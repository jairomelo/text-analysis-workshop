[
  {
    "objectID": "03-basicTextAnalysis.html",
    "href": "03-basicTextAnalysis.html",
    "title": "Basic Text Analysis",
    "section": "",
    "text": "Now that we have our clean, standardized corpus of 26 classic works, we’re ready to begin our text analysis journey. In this episode, we’ll explore fundamental text analysis techniques that form the foundation for more sophisticated analyses.\nWe’ll start by examining word frequencies to understand which words appear most often in our texts. Then, we’ll calculate lexical density to measure vocabulary richness across different works and authors. Finally, we’ll explore readability metrics to assess text complexity. These basic analyses will give us our first insights into the linguistic patterns that distinguish different authors, genres, and time periods in our corpus.",
    "crumbs": [
      "3. Basic Text Analysis"
    ]
  },
  {
    "objectID": "03-basicTextAnalysis.html#word-frequency-analysis",
    "href": "03-basicTextAnalysis.html#word-frequency-analysis",
    "title": "Basic Text Analysis",
    "section": "Word Frequency Analysis",
    "text": "Word Frequency Analysis\nWord frequency analysis is one of the most fundamental techniques in text analysis. It helps us understand which words appear most often in our texts and can reveal important patterns about content, style, and themes.\n\nBasic Word Counts\nLet’s start by tokenizing our texts and examining the most frequent words across our entire sample corpus:\n\n# Tokenize all texts and count word frequencies\ncorpus_words &lt;- corpus_texts %&gt;%\n  select(author, title, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  count(word, sort = TRUE)\n\n# Display the top 20 most frequent words\ncorpus_words %&gt;%\n  slice_head(n = 20)\n\n# A tibble: 20 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 the   145089\n 2 and    93833\n 3 to     73111\n 4 of     72902\n 5 a      59253\n 6 i      54193\n 7 in     44105\n 8 was    36577\n 9 it     34497\n10 that   32877\n11 he     30956\n12 his    25871\n13 you    24998\n14 with   23558\n15 as     22959\n16 her    22678\n17 had    22195\n18 for    21130\n19 she    19803\n20 at     18761\n\n\nAs expected, the most frequent words are stop words (function words like “the,” “and,” “to”). While these words are crucial for language structure, they don’t tell us much about the content or themes of our texts.\n\n\nRemoving Stop Words\nLet’s remove stop words to focus on content words that carry more meaning:\n\n# Remove stop words and examine content words\ncontent_words &lt;- corpus_texts %&gt;%\n  select(author, title, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  count(word, sort = TRUE)\n\n# Display the top 20 most frequent content words\ncontent_words %&gt;%\n  slice_head(n = 20)\n\n# A tibble: 20 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 time      4976\n 2 miss      3302\n 3 dont      3147\n 4 sir       2919\n 5 day       2786\n 6 looked    2614\n 7 eyes      2603\n 8 hand      2557\n 9 head      2394\n10 pickwick  2370\n11 dear      2098\n12 replied   2082\n13 life      1952\n14 night     1946\n15 found     1907\n16 house     1859\n17 round     1850\n18 door      1814\n19 mind      1782\n20 heard     1731\n\n\nNow we can see more meaningful words that give us insights into the themes and content of our corpus.\n\n\nVisualizing Word Frequencies\nLet’s create a visualization of the most frequent words:\n\ncontent_words %&gt;%\n  slice_head(n = 15) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(x = word, y = n)) +\n  geom_col(fill = \"steelblue\", alpha = 0.8) +\n  coord_flip() +\n  labs(\n    title = \"Most Frequent Words in Our Literary Corpus\",\n    subtitle = \"Stop words removed\",\n    x = \"Words\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nAuthor-Specific Word Frequencies\nNow let’s examine how word usage varies by author. This can reveal distinctive vocabulary patterns:\n\n# Calculate word frequencies by author\nauthor_words &lt;- corpus_texts %&gt;%\n  select(author, title, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  count(author, word, sort = TRUE) %&gt;%\n  group_by(author) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  ungroup()\n\n# Visualize top words by author\nauthor_words %&gt;%\n  ggplot(aes(x = reorder_within(word, n, author), y = n, fill = author)) +\n  geom_col() +\n  scale_x_reordered() +\n  coord_flip() +\n  facet_wrap(~author, scales = \"free\") +\n  labs(\n    title = \"Most Frequent Words by Author\",\n    x = \"Words\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "3. Basic Text Analysis"
    ]
  },
  {
    "objectID": "03-basicTextAnalysis.html#lexical-density-analysis",
    "href": "03-basicTextAnalysis.html#lexical-density-analysis",
    "title": "Basic Text Analysis",
    "section": "Lexical Density Analysis",
    "text": "Lexical Density Analysis\nLexical density measures the richness and variety of vocabulary in a text. It’s calculated as the ratio of unique words to total words, often expressed as a percentage:\n\\[\n\\text{Lexical Density} = \\frac{\\text{Number of Unique Words}}{\\text{Total Number of Words}} \\times 100\n\\]\nHigher lexical density indicates more varied vocabulary, while lower density suggests more repetitive language.\n\nCalculating Lexical Density\n\n# Calculate lexical density for each text\nlexical_stats &lt;- corpus_texts %&gt;%\n  select(author, title, year, genre, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  group_by(author, title, year, genre) %&gt;%\n  summarise(\n    total_words = n(),\n    unique_words = n_distinct(word),\n    lexical_density = (unique_words / total_words) * 100,\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(lexical_density))\n\nlexical_stats\n\n# A tibble: 25 × 7\n   author             title  year genre total_words unique_words lexical_density\n   &lt;chr&gt;              &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;        &lt;int&gt;           &lt;dbl&gt;\n 1 H. G. Wells        The …  1895 Scie…       11162         4347            38.9\n 2 F. Scott Fitzgera… The …  1925 Soci…       17432         5775            33.1\n 3 Kenneth Grahame    The …  1908 Fant…       20786         6618            31.8\n 4 Jonathan Swift     Gull…  1726 Sati…       18142         5529            30.5\n 5 E. M. Forster      A Ro…  1908 Soci…       22844         6909            30.2\n 6 Mark Twain         Tom …  1876 Adve…       25728         7772            30.2\n 7 J. M. Barrie       Pete…  1911 Fant…       15199         4465            29.4\n 8 Elizabeth Gaskell  Cran…  1853 Dome…       23296         6680            28.7\n 9 Robert Louis Stev… Trea…  1883 Adve…       22647         6068            26.8\n10 Jules Verne        Arou…  1873 Adve…       24914         6631            26.6\n# ℹ 15 more rows\n\n\n\n\nVisualizing Lexical Density\n\nlexical_stats %&gt;%\n  mutate(title = str_wrap(title, 20)) %&gt;%\n  ggplot(aes(x = reorder(title, lexical_density), y = lexical_density, fill = genre)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Lexical Density by Work\",\n    subtitle = \"Percentage of unique words (stop words excluded)\",\n    x = \"Work\",\n    y = \"Lexical Density (%)\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExploring Relationships\nLet’s examine how lexical density relates to text length and publication year:\n\n# Lexical density vs. text length\nlexical_stats %&gt;%\n  ggplot(aes(x = total_words, y = lexical_density)) +\n  geom_point(aes(color = genre), size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray50\", linetype = \"dashed\") +\n  geom_text(aes(label = str_wrap(title, 15)), vjust = -0.5, size = 3) +\n  labs(\n    title = \"Lexical Density vs. Text Length\",\n    x = \"Total Words (excluding stop words)\",\n    y = \"Lexical Density (%)\",\n    color = \"Genre\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Lexical density vs. publication year\nlexical_stats %&gt;%\n  ggplot(aes(x = year, y = lexical_density)) +\n  geom_point(aes(color = genre), size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray50\", linetype = \"dashed\") +\n  geom_text(aes(label = str_wrap(title, 15)), vjust = -0.5, size = 3) +\n  labs(\n    title = \"Lexical Density Over Time\",\n    x = \"Publication Year\",\n    y = \"Lexical Density (%)\",\n    color = \"Genre\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "3. Basic Text Analysis"
    ]
  },
  {
    "objectID": "03-basicTextAnalysis.html#basic-readability-metrics",
    "href": "03-basicTextAnalysis.html#basic-readability-metrics",
    "title": "Basic Text Analysis",
    "section": "Basic Readability Metrics",
    "text": "Basic Readability Metrics\nReadability metrics help us assess how easy or difficult a text is to read. While these metrics have limitations, they provide useful baseline measurements for comparing texts.\n\nAverage Word Length\nOne simple measure of text complexity is average word length:\n\n# Calculate average word length per text\nword_length_stats &lt;- corpus_texts %&gt;%\n  select(author, title, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  mutate(word_length = nchar(word)) %&gt;%\n  group_by(author, title) %&gt;%\n  summarise(\n    avg_word_length = mean(word_length),\n    median_word_length = median(word_length),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(avg_word_length))\n\nword_length_stats\n\n# A tibble: 25 × 4\n   author                      title          avg_word_length median_word_length\n   &lt;chr&gt;                       &lt;chr&gt;                    &lt;dbl&gt;              &lt;dbl&gt;\n 1 Jules Verne                 Twenty Thousa…            4.63                  4\n 2 Jules Verne                 Around the Wo…            4.57                  4\n 3 Herman Melville             Moby Dick                 4.49                  4\n 4 Charles Dickens             The Pickwick …            4.47                  4\n 5 Jules Verne                 A Journey to …            4.44                  4\n 6 Jane Austen                 Pride and Pre…            4.43                  4\n 7 Mary Wollstonecraft Shelley Frankenstein;…            4.43                  4\n 8 Jane Austen                 Northanger Ab…            4.41                  4\n 9 Elizabeth Von Arnim         The Enchanted…            4.38                  4\n10 H. G. Wells                 The Time Mach…            4.36                  4\n# ℹ 15 more rows\n\n\n\n\nSentence Length Analysis\nWe can also estimate sentence complexity by analyzing sentence lengths:\n\n# Simple sentence splitting and length calculation\nsentence_stats &lt;- corpus_texts %&gt;%\n  select(author, title, text) %&gt;%\n  mutate(\n    # Simple sentence splitting (this is crude but functional)\n    sentences = str_split(text, \"[.!?]+\")\n  ) %&gt;%\n  unnest(sentences) %&gt;%\n  mutate(\n    sentences = str_trim(sentences),\n    word_count = str_count(sentences, \"\\\\S+\")\n  ) %&gt;%\n  filter(word_count &gt; 0) %&gt;%\n  group_by(author, title) %&gt;%\n  summarise(\n    avg_sentence_length = mean(word_count),\n    median_sentence_length = median(word_count),\n    total_sentences = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(avg_sentence_length))\n\nsentence_stats\n\n# A tibble: 25 × 5\n   author       title avg_sentence_length median_sentence_length total_sentences\n   &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;                  &lt;dbl&gt;           &lt;int&gt;\n 1 Jonathan Sw… Gull…                28.2                     23            1851\n 2 Elizabeth G… Cran…                25.5                     20            2806\n 3 Mary Wollst… Fran…                22.3                     20            3378\n 4 Herman Melv… Moby…                20.0                     14           10717\n 5 Jane Austen  Nort…                19.8                     14            3952\n 6 Louisa May … Litt…                19.6                     17            9943\n 7 Kenneth Gra… The …                18.2                     13            3269\n 8 Jane Austen  Prid…                17.5                     13            7356\n 9 Robert Loui… Trea…                17.5                     13            3950\n10 Bram Stoker  Drac…                17.4                     15            9331\n# ℹ 15 more rows\n\n\n\n\nCombined Readability Visualization\nLet’s create a comprehensive view of our text complexity metrics:\n\n# Combine our metrics\nreadability_combined &lt;- lexical_stats %&gt;%\n  left_join(word_length_stats, by = c(\"author\", \"title\")) %&gt;%\n  left_join(sentence_stats, by = c(\"author\", \"title\"))\n\n# Create a multi-faceted visualization\nreadability_long &lt;- readability_combined %&gt;%\n  select(title, lexical_density, avg_word_length, avg_sentence_length) %&gt;%\n  pivot_longer(cols = -title, names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(\n    title = str_wrap(title, 20),\n    metric = case_when(\n      metric == \"lexical_density\" ~ \"Lexical Density (%)\",\n      metric == \"avg_word_length\" ~ \"Avg Word Length\",\n      metric == \"avg_sentence_length\" ~ \"Avg Sentence Length\"\n    )\n  )\n\nreadability_long %&gt;%\n  ggplot(aes(x = reorder(title, value), y = value)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  coord_flip() +\n  facet_wrap(~metric, scales = \"free_x\") +\n  labs(\n    title = \"Text Complexity Metrics Across Our Corpus\",\n    x = \"Work\",\n    y = \"Value\"\n  ) +\n  theme_minimal() +\n  theme(strip.text = element_text(size = 12, face = \"bold\"))",
    "crumbs": [
      "3. Basic Text Analysis"
    ]
  },
  {
    "objectID": "01-basicConcepts.html",
    "href": "01-basicConcepts.html",
    "title": "What is Text Analysis?",
    "section": "",
    "text": "Text analysis is an umbrella concept that involves multiple techniques, methods, and approaches for “extracting” the meaning, structure, or general characteristics of a text by analyzing its constitutive words and symbols, and their relationships with a context, epoch, trend, intention, etc.\nThanks to the massification of computers and the miniaturization of computer power, computational methods for text analysis have become prevalent in certain contexts, allowing researchers to analyze large corpora of texts and also extrapolate those concepts for purposes beyond academic research, such as commercial text processing, sentiment analysis, or information retrieval.\nBuilding on these foundations, this workshop focuses on the basic processes required to prepare a corpus of texts for analysis, and applies introductory analytical techniques that establish common ground for more complex tasks such as sentiment analysis, language modeling, topic modeling, or text generation.",
    "crumbs": [
      "1. Basic Concepts"
    ]
  },
  {
    "objectID": "01-basicConcepts.html#text-analysis-fundamentals",
    "href": "01-basicConcepts.html#text-analysis-fundamentals",
    "title": "What is Text Analysis?",
    "section": "Text Analysis Fundamentals",
    "text": "Text Analysis Fundamentals\nTo introduce the most basic tasks of text analysis, let’s use as an example the short story by Franz Kafka, “Before the Law”, originally published in 1915. We’re going to use Ian Johnston’s translation as our working text.\n\nBEFORE THE LAW\nBefore the law sits a gatekeeper. To this gatekeeper comes a man from the country who asks to gain entry into the law. But the gatekeeper says that he cannot grant him entry at the moment. The man thinks about it and then asks if he will be allowed to come in sometime later on. “It is possible,” says the gatekeeper, “but not now.” The gate to the law stands open, as always, and the gatekeeper walks to the side, so the man bends over in order to see through the gate into the inside. When the gatekeeper notices that, he laughs and says: “If it tempts you so much, try going inside in spite of my prohibition. But take note. I am powerful. And I am only the lowliest gatekeeper. But from room to room stand gatekeepers, each more powerful than the last. I cannot endure even one glimpse of the third.” The man from the country has not expected such difficulties: the law should always be accessible for everyone, he thinks, but as he now looks more closely at the gatekeeper in his fur coat, at his large pointed nose and his long, thin, black Tartar’s beard, he decides that it would be better to wait until he gets permission to go inside. The gatekeeper gives him a stool and allows him to sit down at the side in front of the gate. There he sits for days and years. He makes many attempts to be let in, and he wears the gatekeeper out with his requests. The gatekeeper often interrogates him briefly, questioning him about his homeland and many other things, but they are indifferent questions, the kind great men put, and at the end he always tells him once more that he cannot let him inside yet. The man, who has equipped himself with many things for his journey, spends everything, no matter how valuable, to win over the gatekeeper. The latter takes it all but, as he does so, says, “I am taking this only so that you do not think you have failed to do anything.” During the many years the man observes the gatekeeper almost continuously. He forgets the other gatekeepers, and this first one seems to him the only obstacle for entry into the law. He curses the unlucky circumstance, in the first years thoughtlessly and out loud; later, as he grows old, he only mumbles to himself. He becomes childish and, since in the long years studying the gatekeeper he has also come to know the fleas in his fur collar, he even asks the fleas to help him persuade the gatekeeper. Finally his eyesight grows weak, and he does not know whether things are really darker around him or whether his eyes are merely deceiving him. But he recognizes now in the darkness an illumination which breaks inextinguishably out of the gateway to the law. Now he no longer has much time to live. Before his death he gathers up in his head all his experiences of the entire time into one question which he has not yet put to the gatekeeper. He waves to him, since he can no longer lift up his stiffening body. The gatekeeper has to bend way down to him, for the difference between them has changed considerably to the disadvantage of the man. “What do you want to know now?” asks the gatekeeper. “You are insatiable.” “Everyone strives after the law,” says the man, “so how is it that in these many years no one except me has requested entry?” The gatekeeper sees that the man is already dying and, in order to reach his diminishing sense of hearing, he shouts at him, “Here no one else can gain entry, since this entrance was assigned only to you. I’m going now to close it.\n\nNow, let’s perform a very basic task in text analysis: calculate the length of the text. Take your time and count how many words are in this text, without the title.\nAs a second task, can you identify the most frequently used word in the text? You can ignore common stop words (e.g., “the”, “and”, “is”, etc.) for this task.\nEvidently, doing this type of task manually is tedious and error-prone. However, this is how text analysis was originally performed. Nowadays, these tasks can be automated using various tools and programming languages, such as Python or R.\nLet’s use R to perform these same tasks more efficiently with plain R code. We’ll use the strsplit() function to split the text into words, and then count them using the lengths() function.”\n\nbeforethelaw &lt;- \"Before the law sits a gatekeeper. To this gatekeeper comes a man from the country who asks to gain entry into the law. But the gatekeeper says that he cannot grant him entry at the moment. The man thinks about it and then asks if he will be allowed to come in sometime later on. “It is possible,” says the gatekeeper, “but not now.” The gate to the law stands open, as always, and the gatekeeper walks to the side, so the man bends over in order to see through the gate into the inside. When the gatekeeper notices that, he laughs and says: “If it tempts you so much, try going inside in spite of my prohibition. But take note. I am powerful. And I am only the lowliest gatekeeper. But from room to room stand gatekeepers, each more powerful than the last. I cannot endure even one glimpse of the third.” The man from the country has not expected such difficulties: the law should always be accessible for everyone, he thinks, but as he now looks more closely at the gatekeeper in his fur coat, at his large pointed nose and his long, thin, black Tartar’s beard, he decides that it would be better to wait until he gets permission to go inside. The gatekeeper gives him a stool and allows him to sit down at the side in front of the gate. There he sits for days and years. He makes many attempts to be let in, and he wears the gatekeeper out with his requests. The gatekeeper often interrogates him briefly, questioning him about his homeland and many other things, but they are indifferent questions, the kind great men put, and at the end he always tells him once more that he cannot let him inside yet. The man, who has equipped himself with many things for his journey, spends everything, no matter how valuable, to win over the gatekeeper. The latter takes it all but, as he does so, says, “I am taking this only so that you do not think you have failed to do anything.” During the many years the man observes the gatekeeper almost continuously. He forgets the other gatekeepers, and this first one seems to him the only obstacle for entry into the law. He curses the unlucky circumstance, in the first years thoughtlessly and out loud; later, as he grows old, he only mumbles to himself. He becomes childish and, since in the long years studying the gatekeeper he has also come to know the fleas in his fur collar, he even asks the fleas to help him persuade the gatekeeper. Finally his eyesight grows weak, and he does not know whether things are really darker around him or whether his eyes are merely deceiving him. But he recognizes now in the darkness an illumination which breaks inextinguishably out of the gateway to the law. Now he no longer has much time to live. Before his death he gathers up in his head all his experiences of the entire time into one question which he has not yet put to the gatekeeper. He waves to him, since he can no longer lift up his stiffening body. The gatekeeper has to bend way down to him, for the difference between them has changed considerably to the disadvantage of the man. “What do you want to know now?” asks the gatekeeper. “You are insatiable.” “Everyone strives after the law,” says the man, “so how is it that in these many years no one except me has requested entry?” The gatekeeper sees that the man is already dying and, in order to reach his diminishing sense of hearing, he shouts at him, “Here no one else can gain entry, since this entrance was assigned only to you. I’m going now to close it.\"\n\n# Split the string into words based on whitespace\nword_list &lt;- strsplit(beforethelaw, \"\\\\W+\") # Split by non-word characters\n\n# Count the words\nword_count &lt;- lengths(word_list)\n\nword_count\n\n[1] 642\n\n\nEven this apparently short text has 640 words, which would be quite tedious to count manually. In seconds, R has accomplished what might have taken us considerable time and effort by hand.\nNow, let’s find the most frequently used word in the text. We’ll use the table() function to count word frequencies, then identify the most common word:\n\nword_counts &lt;- table(unlist(word_list))\n\nmost_frequent_word &lt;- names(word_counts)[which.max(word_counts)]\n\nmost_frequent_word\n\n[1] \"the\"\n\n\nThe most frequently used word in the text is “the”, which appears 20 times. This is a common stop word in English, so let’s remove it and find the next most frequent word. Rather than manually creating our own list of stop words, we can use an existing comprehensive list prepared for this purpose, such as the one provided by stopwords ISO. We’ll use their English stopwords file: https://raw.githubusercontent.com/stopwords-iso/stopwords-en/refs/heads/master/stopwords-en.txt. ou don’t need to download this file, as we can read it directly from the web using the readLines() function.\n\n# Read the stop words from the web\nstop_words_url &lt;- \"https://raw.githubusercontent.com/stopwords-iso/stopwords-en/refs/heads/master/stopwords-en.txt\"\nstop_words &lt;- readLines(stop_words_url)\n\nWarning in readLines(stop_words_url): incomplete final line found on\n'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/refs/heads/master/stopwords-en.txt'\n\n# Remove common stop words\nword_counts_filtered &lt;- word_counts[!names(word_counts) %in% stop_words]\nmost_frequent_word_filtered &lt;- names(word_counts_filtered)[which.max(word_counts_filtered)]\nmost_frequent &lt;- word_counts_filtered[most_frequent_word_filtered]\nmost_frequent\n\ngatekeeper \n        19 \n\n\nAnd now we have a winner! The most frequently used word in the text, after removing common stop words, is “gatekeeper”, which appears 12 times. In this text, the “gatekeeper” is not merely a character, but represents the central theme of the story. The gatekeeper serves as a complex metaphor for the complexities of the law and the seemingly infinite barriers that prevent ordinary people from accessing it—without suggesting that the law is inherently closed to them (which is why the gate remains open throughout). The intricacies of this complex story are beyond the scope of this workshop, but it is interesting to see how a simple frequency analysis can reveal the thematic core of a text.\n\n\n\n\n\n\nCross-linguistic Analysis\n\n\n\nInterestingly, performing the same analysis on Kafka’s original German text reveals “Türhüter” (gatekeeper) as the most frequent non-stopword, matching our English results. This consistency across languages suggests that frequency analysis can reliably identify thematically central terms, even accounting for translation differences.\n\n\nThis is a very basic example where we implemented each step manually to demonstrate the underlying processes. However, in practice, after decades of computational text analysis development, there are many specialized packages and libraries that can perform these tasks more efficiently. For example, in R, we can use the tidytext package to handle text data in a tidy format, making it easier to manipulate and analyze.\nIn the next episode, we will explore how to create a corpus of texts and prepare it for analysis. We will also cover some basic preprocessing steps, such as removing punctuation, converting text to lowercase, and tokenizing the text into words or sentences. This will establish the foundation for more advanced text analysis techniques that we will explore later in this workshop.",
    "crumbs": [
      "1. Basic Concepts"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Analysis with R Workshop",
    "section": "",
    "text": "This workshop introduces fundamental concepts and techniques for computational text analysis using R. You’ll learn to prepare, analyze, and extract insights from textual data through hands-on exercises with real texts.\n\n\n\nBasic concepts of text analysis and corpus creation\nText preprocessing and quality assessment techniques\n\nTF-IDF analysis for identifying important terms\nSimilarity measures for comparing texts\n\n\n\n\n\nBasic Concepts - Understanding text analysis fundamentals\nCreating a Corpus - Building your text collection\nBasic Text Analysis - Core analytical techniques\nText Quality - Assessing and improving text data\nTF-IDF Analysis - Term frequency and importance\nFinding Similarities - Comparing texts\n\n\n\n\nTBD",
    "crumbs": [
      "Summary and Setup"
    ]
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Text Analysis with R Workshop",
    "section": "",
    "text": "Basic concepts of text analysis and corpus creation\nText preprocessing and quality assessment techniques\n\nTF-IDF analysis for identifying important terms\nSimilarity measures for comparing texts",
    "crumbs": [
      "Summary and Setup"
    ]
  },
  {
    "objectID": "index.html#workshop-structure",
    "href": "index.html#workshop-structure",
    "title": "Text Analysis with R Workshop",
    "section": "",
    "text": "Basic Concepts - Understanding text analysis fundamentals\nCreating a Corpus - Building your text collection\nBasic Text Analysis - Core analytical techniques\nText Quality - Assessing and improving text data\nTF-IDF Analysis - Term frequency and importance\nFinding Similarities - Comparing texts",
    "crumbs": [
      "Summary and Setup"
    ]
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Text Analysis with R Workshop",
    "section": "",
    "text": "TBD",
    "crumbs": [
      "Summary and Setup"
    ]
  },
  {
    "objectID": "04-textQuality.html",
    "href": "04-textQuality.html",
    "title": "Text Quality",
    "section": "",
    "text": "In the previous chapter we performed some basic text analysis techniques which helped us to understand the general shape of our corpus. We found the most frequent words in our corpus, what words were more frequently used by each author, and we performed a lexical density analysis to calculate how rich the vocabulary is in each text, and we compared with basic readability metrics to explore how lexical complexity and readability can influence each other.\nIn this episode, we’re going to return to our starting example of Kafka’s story, and we’re going to explore how some of these measurements can actually help us to identify if a text is ready for analysis or if it requires further preprocessing. This approach can be particularly helpful when there’s uncertainty about the quality of the textual data (e.g., it proceeds from poor OCR, transcription errors, or social media sources).\nWe have prepared for this exercise three versions of the same text:\nOne important note here is that all examples have been taken from real sources, which is an indicator of what we can expect to find in real text analysis scenarios, particularly when working with historical digitizations or OCR outputs where quality can vary dramatically.",
    "crumbs": [
      "4. Text Quality"
    ]
  },
  {
    "objectID": "04-textQuality.html#lexical-density-as-quality-indicator",
    "href": "04-textQuality.html#lexical-density-as-quality-indicator",
    "title": "Text Quality",
    "section": "Lexical Density as Quality Indicator",
    "text": "Lexical Density as Quality Indicator\nLet’s start performing a lexical density analysis, as we did previously for our corpus, to see how these different versions compare in terms of their lexical richness.\n\nlibrary(tidyverse)\nlibrary(tidytext)\n\n\n# Load the text data\nkafka &lt;- tribble(\n    ~version, ~url,\n    \"clean\", \"texts/Kafka-beforethelaw.txt\",\n    \"messy\", \"texts/Kafka-btl-messy.txt\",\n    \"dirty\", \"texts/Kafka-btl-dirty.txt\"\n)\n\n# Download the text data\nkafka_texts &lt;- kafka %&gt;%\n    mutate(text = map_chr(url, ~ readLines(.x, warn = FALSE) %&gt;% paste(collapse = \" \")))\n\nkafka_texts\n\n# A tibble: 3 × 3\n  version url                          text                                     \n  &lt;chr&gt;   &lt;chr&gt;                        &lt;chr&gt;                                    \n1 clean   texts/Kafka-beforethelaw.txt \"BEFORE THE LAW  Before the law sits a g…\n2 messy   texts/Kafka-btl-messy.txt    \"Before the Law stands a doorkeeper. To …\n3 dirty   texts/Kafka-btl-dirty.txt    \"Befotteh eL aws tandas doorkeepTero.t h…\n\n\nAfter loading our text data, we can now proceed to analyze its lexical density.\n\n# Tokenize and count words \nkafka_word_counts &lt;- kafka_texts %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    anti_join(stop_words) %&gt;% # Remove stop words to match with previous analysis\n    count(version, word)\n\nJoining with `by = join_by(word)`\n\n# Calculate lexical density\nkafka_lexical_density &lt;- kafka_word_counts %&gt;%\n    group_by(version) %&gt;%\n    summarise(\n        total_words = sum(n),\n        unique_words = n_distinct(word),\n        lexical_density = (n_distinct(word) / sum(n)) * 100) %&gt;%\n    arrange(lexical_density)\n\nkafka_lexical_density\n\n# A tibble: 3 × 4\n  version total_words unique_words lexical_density\n  &lt;chr&gt;         &lt;int&gt;        &lt;int&gt;           &lt;dbl&gt;\n1 clean           165          122            73.9\n2 messy           189          154            81.5\n3 dirty           336          315            93.8\n\n\nOur sample is small enough to be understandable without requiring any visualization. Even using the same text, it’s possible to see how the lexical density varies across different versions. In some way, we can adventure the hypothesis that noise in text data is equivalent to a high lexical density. In our case, the clean text has a lexical density of 73.9% while the dirty text has a lexical density of 93.8%.\nSo, let set the lexical variation as the level of noise in the text. Additionaly, we can perform other analyses to further investigate the quality of our texts, such as finding the coefficient of variation, the Shannon entropy, and visualize the Zipf distribution.",
    "crumbs": [
      "4. Text Quality"
    ]
  },
  {
    "objectID": "04-textQuality.html#coefficient-of-variation",
    "href": "04-textQuality.html#coefficient-of-variation",
    "title": "Text Quality",
    "section": "Coefficient of Variation",
    "text": "Coefficient of Variation\nThe coefficient of variation (CV) measures how much variability exists relative to the average value. It is calculated as the ratio of the standard deviation to the mean, and is often expressed as a percentage. A higher CV indicates greater variability in relation to the mean.\n\n# Calculate coefficient of variation and combine with lexical density metrics\nkafka_cv &lt;- kafka_word_counts %&gt;%\n    group_by(version) %&gt;%\n    summarise(\n        mean_freq = mean(n),\n        sd_freq = sd(n),\n        cv = (sd_freq / mean_freq) * 100,\n        .groups = 'drop'\n    )\n\n# Combine with lexical density data to have the full picture\nkafka_metrics &lt;- kafka_lexical_density %&gt;%\n    left_join(kafka_cv, by = \"version\") %&gt;%\n    arrange(lexical_density)\n\nkafka_metrics\n\n# A tibble: 3 × 7\n  version total_words unique_words lexical_density mean_freq sd_freq    cv\n  &lt;chr&gt;         &lt;int&gt;        &lt;int&gt;           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 clean           165          122            73.9      1.35   1.81  134. \n2 messy           189          154            81.5      1.23   1.52  124. \n3 dirty           336          315            93.8      1.07   0.421  39.4\n\n\nThese results are already revealing an interesting pattern. As the text quality deteriorates, lexical density increases dramatically and the coefficient of variation decreases. In clean text, you have natural language patterns where some words appear much more frequently than others (high CV = 134%). In dirty text, most words appear only once due to OCR creating unique “words” from errors (low CV = 39.4%). In consequence, a lower CV suggests worse quality. We can visualize this trend by calculating the Zipfian distribution for each version.",
    "crumbs": [
      "4. Text Quality"
    ]
  },
  {
    "objectID": "04-textQuality.html#zipfian-distribution",
    "href": "04-textQuality.html#zipfian-distribution",
    "title": "Text Quality",
    "section": "Zipfian Distribution",
    "text": "Zipfian Distribution\nZipf’s law describes the frequency of terms in a language, where a few terms are very common while many others are rare. According to this law, the frequency of a word is inversely proportional to its rank in the frequency table. We can visualize the distribution of term frequencies to see if our texts follow this pattern. A Zipfian distribution indicates that a small number of terms dominate usage, while many others are used infrequently—a hallmark of natural language.\nZipfian analysis typically combines two different measurements: theoretical predictions based on Zipf’s law and empirical observations from our actual data. Comparing these can give us a quantitative sense of how close to natural language patterns each text version is.\nLet’s first calculate and visualize these distributions:\n\n# Calculate empirical and theoretical Zipf distributions\nkafka_zipf &lt;- kafka_word_counts %&gt;%\n  group_by(version) %&gt;%\n  arrange(desc(n)) %&gt;%\n  mutate(\n    empirical_rank = row_number(),\n    total_words_version = sum(n),\n    empirical_prob = n / total_words_version\n  ) %&gt;%\n  mutate(\n    raw_zipf = 1 / empirical_rank,\n    zipf_sum = sum(raw_zipf),\n    theoretical_prob = raw_zipf / zipf_sum\n  ) %&gt;%\n  ungroup()\n\nkafka_zipf\n\n# A tibble: 591 × 9\n   version word           n empirical_rank total_words_version empirical_prob\n   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;               &lt;int&gt;          &lt;dbl&gt;\n 1 clean   gatekeeper    19              1                 165        0.115  \n 2 messy   doorkeeper    19              1                 189        0.101  \n 3 clean   law            8              2                 165        0.0485 \n 4 dirty   doorkeeper     7              1                 336        0.0208 \n 5 messy   law            6              2                 189        0.0317 \n 6 clean   entry          5              3                 165        0.0303 \n 7 clean   inside         4              4                 165        0.0242 \n 8 clean   gate           3              5                 165        0.0182 \n 9 dirty   hed            3              2                 336        0.00893\n10 dirty   law            3              3                 336        0.00893\n# ℹ 581 more rows\n# ℹ 3 more variables: raw_zipf &lt;dbl&gt;, zipf_sum &lt;dbl&gt;, theoretical_prob &lt;dbl&gt;\n\n# Create the Zipf visualization\nkafka_zipf %&gt;%\n  ggplot(aes(x = empirical_rank)) +\n  geom_line(aes(y = empirical_prob, color = \"Empirical\"), linewidth = 1) +\n  geom_line(aes(y = theoretical_prob, color = \"Theoretical Zipf\"), linetype = \"dashed\", linewidth = 1) +\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"Empirical\" = \"steelblue\", \"Theoretical Zipf\" = \"red\")) +\n  facet_wrap(~version, scales = \"free\", labeller = labeller(version = str_to_title)) +\n  labs(\n    title = \"Empirical Rank-Frequency vs Theoretical Zipf\",\n    subtitle = \"Comparing observed word frequencies with theoretical Zipf distribution\",\n    x = \"Rank (descending frequency)\",\n    y = \"Probability (normalized frequency)\",\n    color = \"Distribution Type\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor = element_line(linetype = \"dotted\", color = \"grey80\"),\n    strip.text = element_text(size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nWe can observe how the clean text closely follows the expected Zipfian curve for the most frequent words, with the characteristic steep decline that defines natural language patterns. The messy text shows moderate deviation from the theoretical curve, particularly in the middle ranks, while still maintaining some adherence to Zipfian structure. The dirty text exhibits the most dramatic departure from the expected pattern—the empirical distribution becomes notably flatter with extended horizontal segments, indicating that OCR errors have created many unique ‘words’ that appear with identical low frequencies. This flattening disrupts the natural frequency hierarchy where a few words should dominate usage while most others remain rare, creating instead an artificial abundance of singleton terms that break the fundamental linguistic structure captured by Zipf’s law.\nWhile these patterns are visually apparent, we can obtain a more precise quantitative measure of “naturalness” by calculating the average Zipf deviation for each text version:\n\n# Calculate Zipf deviation metrics\nkafka_zipf_deviation &lt;- kafka_zipf %&gt;%\n  group_by(version) %&gt;%\n  summarise(\n    # Calculate average deviation between empirical and theoretical probabilities\n    avg_zipf_deviation = mean(abs(empirical_prob - theoretical_prob) / theoretical_prob) * 100,\n    # Spearman correlation between rank and frequency\n    spearman_correlation = cor(empirical_rank, empirical_prob, method = \"spearman\"),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(avg_zipf_deviation)\n\nkafka_zipf_deviation\n\n# A tibble: 3 × 3\n  version avg_zipf_deviation spearman_correlation\n  &lt;chr&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n1 clean                 121.               -0.553\n2 messy                 149.               -0.498\n3 dirty                 214.               -0.345\n\n\nThese numerical results confirm our visual observations and provide precise quality indicators. What we observe is that high-quality text exhibits lower average Zipf deviation and stronger negative Spearman correlation. The dirty text shows dramatically high average Zipf deviation and weak Spearman correlation, indicating significant departure from expected Zipfian distribution. We can observe a clear progression of degradation from clean to dirty text: 121% → 149% → 214% for Zipf deviation, and -0.553 → -0.498 → -0.345 for Spearman correlation.\nIn practical terms, these calculations allow us to quantify the “naturalness” of each text version. For text cleaning workflows, progressively reducing Zipf deviation while increasing the coefficient of variation would help calibrate the fine-tuning of text normalization processes, providing objective metrics for preprocessing decisions.",
    "crumbs": [
      "4. Text Quality"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html",
    "href": "06-findingSimilarities.html",
    "title": "Finding Similarities between Texts",
    "section": "",
    "text": "In our previous episodes, we explored word frequencies, lexical density, readability metrics, and TF-IDF to understand individual characteristics of texts. Now we turn to a fundamental question in computational text analysis: How similar are different texts to each other?\nText similarity is crucial for many applications: detecting plagiarism, finding texts by similar authors, grouping documents by topic, or discovering literary influences. In this episode, we’ll explore two key mathematical approaches for measuring textual similarity: cosine distance and Euclidean distance.",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#understanding-distance-and-similarity",
    "href": "06-findingSimilarities.html#understanding-distance-and-similarity",
    "title": "Finding Similarities between Texts",
    "section": "Understanding Distance and Similarity",
    "text": "Understanding Distance and Similarity\nWhen we measure text similarity, we’re actually measuring how “close” or “distant” texts are in a mathematical space. Each text can be represented as a vector (a list of numbers), where each dimension represents a word and its value represents how important that word is in the text.\nFor this analysis, we’ll use TF-IDF vectors - representing each text by the TF-IDF scores of all words in our corpus. This gives us a nuanced view that considers both word frequency and distinctiveness.\n\nThe Mathematics of Text Distance\nDistance and similarity are inversely related: - Small distance = High similarity (texts are alike) - Large distance = Low similarity (texts are different)\nWe’ll explore two fundamental distance metrics:\n\nCosine Distance: Measures the angle between text vectors (focuses on proportional word usage)\nEuclidean Distance: Measures the straight-line distance between text vectors (considers both proportions and magnitudes)",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#setting-up-our-analysis",
    "href": "06-findingSimilarities.html#setting-up-our-analysis",
    "title": "Finding Similarities between Texts",
    "section": "Setting Up Our Analysis",
    "text": "Setting Up Our Analysis\nLet’s work with our established corpus, focusing on a subset for clearer visualization. We’ll use texts from different genres to see how mathematical similarity aligns with literary categories:\n\n# Select a strategically chosen subset for similarity analysis\n# Based on the cosine similarity matrix, these texts show more variation and interesting patterns\nsimilarity_corpus &lt;- tribble(\n  ~author, ~title, ~year, ~glID, ~genre,\n  # Social Fiction - but with some internal variation\n  \"Jane Austen\", \"Pride and Prejudice\", 1813, 1342, \"Social Fiction\",\n  \"Charles Dickens\", \"Great Expectations\", 1861, 1400, \"Social Fiction\", \n  \"Mark Twain\", \"Tom Sawyer\", 1876, 74, \"Adventure\", # Shows interesting patterns\n  # Gothic - distinctive vocabulary\n  \"Mary Wollstonecraft Shelley\", \"Frankenstein; Or, The Modern Prometheus\", 1818, 84, \"Gothic\",\n  \"Bram Stoker\", \"Dracula\", 1897, 521, \"Gothic\",\n  \"Charlotte Brontë\", \"Jane Eyre\", 1847, 145, \"Gothic\",\n  # Fantasy - should cluster together\n  \"Lewis Carroll\", \"Alice's Adventures in Wonderland\", 1865, 11, \"Fantasy\",\n  \"Kenneth Grahame\", \"The Wind in the Willows\", 1908, 27805, \"Fantasy\",\n  # Science Fiction - technical vocabulary\n  \"Jules Verne\", \"Twenty Thousand Leagues Under the Seas\", 1870, 2488, \"Science Fiction\",\n  \"H. G. Wells\", \"The Time Machine\", 1895, 43, \"Science Fiction\",\n  # Domestic Fiction - should show similarity to social fiction\n  \"Elizabeth Gaskell\", \"Cranford\", 1853, 394, \"Domestic Fiction\",\n  \"Louisa May Alcott\", \"Little Women\", 1868, 37106, \"Domestic Fiction\"\n)\n\n# Download and clean the texts\nsimilarity_texts &lt;- similarity_corpus %&gt;%\n  mutate(text = map(glID, ~gutenberg_download(.x) %&gt;%\n  pull(text) %&gt;%\n  paste(collapse = \" \"))) %&gt;%\n  mutate(\n   text_clean = str_to_lower(text) %&gt;%\n      str_remove_all(\"'s\\\\b\") %&gt;%                    # Remove possessive 's\n      str_remove_all(\"[[:punct:]]\") %&gt;%              # Remove punctuation\n      str_squish()                                    # Remove extra whitespace\n  ) %&gt;%\n  select(author, title, year, genre, text_clean)\n\nDetermining mirror for Project Gutenberg from\nhttps://www.gutenberg.org/robot/harvest.\nUsing mirror http://aleph.gutenberg.org.\n\nprint(paste(\"Loaded\", nrow(similarity_texts), \"texts for similarity analysis\"))\n\n[1] \"Loaded 12 texts for similarity analysis\"",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#creating-tf-idf-vectors-for-similarity",
    "href": "06-findingSimilarities.html#creating-tf-idf-vectors-for-similarity",
    "title": "Finding Similarities between Texts",
    "section": "Creating TF-IDF Vectors for Similarity",
    "text": "Creating TF-IDF Vectors for Similarity\nTo calculate distances between texts, we need to represent each text as a vector in a common space. We’ll create a TF-IDF matrix where: - Each row represents a text - Each column represents a unique word in our corpus - Each cell contains the TF-IDF score for that word in that text\n\n# Create TF-IDF matrix for similarity calculations\ncorpus_words &lt;- similarity_texts %&gt;%\n  select(title, author, genre, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  # Apply lemmatization for better thematic grouping\n  mutate(word = lemmatize_words(word)) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  # Filter out problematic words that could conflict with column names\n  filter(!word %in% c(\"title\", \"author\", \"genre\", \"text\", \"word\")) %&gt;%\n  # Count words and aggregate after lemmatization (some words may become duplicates)\n  count(title, word, sort = TRUE) %&gt;%\n  # Filter out very rare words (appear in fewer than 2 texts) for cleaner analysis\n  group_by(word) %&gt;%\n  filter(n_distinct(title) &gt;= 2) %&gt;%\n  ungroup()\n\n# Check for any remaining duplicates before creating matrix\nduplicate_check &lt;- corpus_words %&gt;%\n  count(title, word) %&gt;%\n  filter(n &gt; 1)\n\nif(nrow(duplicate_check) &gt; 0) {\n  print(\"Warning: Found duplicate title-word combinations:\")\n  print(duplicate_check)\n}\n\n# Calculate TF-IDF scores\ncorpus_tfidf &lt;- corpus_words %&gt;%\n  bind_tf_idf(word, title, n)\n\n# Create TF-IDF matrix (texts as rows, words as columns)\ntfidf_matrix &lt;- corpus_tfidf %&gt;%\n  select(title, word, tf_idf) %&gt;%\n  # Ensure no duplicates by taking the maximum tf_idf if duplicates exist\n  group_by(title, word) %&gt;%\n  summarise(tf_idf = max(tf_idf), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0) %&gt;%\n  column_to_rownames(\"title\") %&gt;%\n  as.matrix()\n\nprint(paste(\"Created TF-IDF matrix with\", nrow(tfidf_matrix), \"texts and\", ncol(tfidf_matrix), \"words\"))\n\n[1] \"Created TF-IDF matrix with 12 texts and 11763 words\"",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#cosine-distance",
    "href": "06-findingSimilarities.html#cosine-distance",
    "title": "Finding Similarities between Texts",
    "section": "Cosine Distance",
    "text": "Cosine Distance\nCosine distance measures the angle between two vectors, focusing on the direction rather than the magnitude. It’s particularly useful for text analysis because it emphasizes proportional word usage patterns rather than text length.\n\nMathematical Foundation\nFor two vectors a and b, cosine similarity is:\n\\[\n\\text{cosine similarity} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| \\times |\\mathbf{b}|}\n\\]\nAnd cosine distance is:\n\\[\n\\text{cosine distance} = 1 - \\text{cosine similarity}\n\\]\nValues range from 0 to 1: - 0: Identical direction (highest similarity) - 1: Completely opposite direction (lowest similarity)\n\n# Calculate cosine distance matrix\ncosine_distance &lt;- function(matrix) {\n  # Normalize each row (document) by its magnitude\n  norms &lt;- sqrt(rowSums(matrix^2))\n  # Avoid division by zero\n  norms[norms == 0] &lt;- 1\n  normalized_matrix &lt;- matrix / norms\n  \n  # Calculate cosine similarity matrix\n  cosine_sim &lt;- normalized_matrix %*% t(normalized_matrix)\n  \n  # Convert to distance (1 - similarity)\n  cosine_dist &lt;- 1 - cosine_sim\n  return(cosine_dist)\n}\n\ncosine_dist_matrix &lt;- cosine_distance(tfidf_matrix)\n\n# Display the cosine distance matrix\nround(cosine_dist_matrix, 3) %&gt;%\n  knitr::kable(caption = \"Cosine Distance Matrix (0 = identical, 1 = completely different)\")\n\n\nCosine Distance Matrix (0 = identical, 1 = completely different)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlice’s Adventures in Wonderland\nCranford\nDracula\nFrankenstein; Or, The Modern Prometheus\nGreat Expectations\nJane Eyre\nLittle Women\nPride and Prejudice\nThe Time Machine\nThe Wind in the Willows\nTom Sawyer\nTwenty Thousand Leagues Under the Seas\n\n\n\n\nAlice’s Adventures in Wonderland\n0.000\n0.995\n0.995\n0.996\n0.997\n0.994\n0.996\n0.998\n0.999\n0.996\n0.996\n0.992\n\n\nCranford\n0.995\n0.000\n0.958\n0.937\n0.971\n0.905\n0.978\n0.937\n0.969\n0.987\n0.973\n0.956\n\n\nDracula\n0.995\n0.958\n0.000\n0.849\n0.970\n0.962\n0.988\n0.977\n0.976\n0.971\n0.951\n0.865\n\n\nFrankenstein; Or, The Modern Prometheus\n0.996\n0.937\n0.849\n0.000\n0.969\n0.912\n0.977\n0.703\n0.914\n0.979\n0.969\n0.880\n\n\nGreat Expectations\n0.997\n0.971\n0.970\n0.969\n0.000\n0.977\n0.982\n0.981\n0.987\n0.990\n0.622\n0.983\n\n\nJane Eyre\n0.994\n0.905\n0.962\n0.912\n0.977\n0.000\n0.921\n0.943\n0.960\n0.986\n0.968\n0.944\n\n\nLittle Women\n0.996\n0.978\n0.988\n0.977\n0.982\n0.921\n0.000\n0.984\n0.994\n0.993\n0.970\n0.970\n\n\nPride and Prejudice\n0.998\n0.937\n0.977\n0.703\n0.981\n0.943\n0.984\n0.000\n0.988\n0.995\n0.986\n0.990\n\n\nThe Time Machine\n0.999\n0.969\n0.976\n0.914\n0.987\n0.960\n0.994\n0.988\n0.000\n0.992\n0.986\n0.979\n\n\nThe Wind in the Willows\n0.996\n0.987\n0.971\n0.979\n0.990\n0.986\n0.993\n0.995\n0.992\n0.000\n0.989\n0.979\n\n\nTom Sawyer\n0.996\n0.973\n0.951\n0.969\n0.622\n0.968\n0.970\n0.986\n0.986\n0.989\n0.000\n0.972\n\n\nTwenty Thousand Leagues Under the Seas\n0.992\n0.956\n0.865\n0.880\n0.983\n0.944\n0.970\n0.990\n0.979\n0.979\n0.972\n0.000\n\n\n\n\n\nLet’s visualize the cosine distances as a heatmap to see patterns:\n\n# Create a more readable heatmap for cosine distances\ncosine_dist_df &lt;- cosine_dist_matrix %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"text1\") %&gt;%\n  pivot_longer(-text1, names_to = \"text2\", values_to = \"cosine_distance\") %&gt;%\n  # Add genre information\n  left_join(similarity_texts %&gt;% select(title, genre) %&gt;% rename(text1 = title, genre1 = genre), by = \"text1\") %&gt;%\n  left_join(similarity_texts %&gt;% select(title, genre) %&gt;% rename(text2 = title, genre2 = genre), by = \"text2\") %&gt;%\n  # Create shorter titles for better visualization\n  mutate(\n    text1_short = case_when(\n      str_detect(text1, \"Pride\") ~ \"Pride & Prejudice\",\n      str_detect(text1, \"Frankenstein\") ~ \"Frankenstein\",\n      str_detect(text1, \"Alice\") ~ \"Alice in Wonderland\",\n      str_detect(text1, \"Twenty\") ~ \"20,000 Leagues\",\n      str_detect(text1, \"Treasure\") ~ \"Treasure Island\",\n      str_detect(text1, \"Dracula\") ~ \"Dracula\",\n      str_detect(text1, \"Time Machine\") ~ \"Time Machine\",\n      str_detect(text1, \"Wind\") ~ \"Wind in the Willows\",\n      str_detect(text1, \"Gatsby\") ~ \"Great Gatsby\",\n      str_detect(text1, \"Great Expectations\") ~ \"Great Expectations\",\n      TRUE ~ text1\n    ),\n    text2_short = case_when(\n      str_detect(text2, \"Pride\") ~ \"Pride & Prejudice\",\n      str_detect(text2, \"Frankenstein\") ~ \"Frankenstein\",\n      str_detect(text2, \"Alice\") ~ \"Alice in Wonderland\",\n      str_detect(text2, \"Twenty\") ~ \"20,000 Leagues\",\n      str_detect(text2, \"Treasure\") ~ \"Treasure Island\",\n      str_detect(text2, \"Dracula\") ~ \"Dracula\",\n      str_detect(text2, \"Time Machine\") ~ \"Time Machine\",\n      str_detect(text2, \"Wind\") ~ \"Wind in the Willows\",\n      str_detect(text2, \"Gatsby\") ~ \"Great Gatsby\",\n      str_detect(text2, \"Great Expectations\") ~ \"Great Expectations\",\n      TRUE ~ text2\n    )\n  )\n\nggplot(cosine_dist_df, aes(x = text1_short, y = text2_short, fill = cosine_distance)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", \n                       midpoint = 0.5, name = \"Cosine\\nDistance\") +\n  labs(\n    title = \"Cosine Distance Between Texts\",\n    subtitle = \"Blue = More Similar, Red = More Different\",\n    x = \"Text 1\", y = \"Text 2\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n    axis.text.y = element_text(size = 9),\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nNow let’s find the most and least similar text pairs using cosine distance:\n\n# Find most similar and least similar pairs (excluding self-comparisons)\ncosine_pairs &lt;- cosine_dist_df %&gt;%\n  filter(text1 != text2) %&gt;%\n  # Remove duplicate pairs (A-B and B-A are the same)\n  filter(text1 &lt; text2) %&gt;%\n  arrange(cosine_distance)\n\n# Most similar pairs\ncat(\"MOST SIMILAR TEXTS (Cosine Distance):\\n\")\n\nMOST SIMILAR TEXTS (Cosine Distance):\n\nhead(cosine_pairs, 5) %&gt;%\n  select(text1_short, text2_short, cosine_distance, genre1, genre2) %&gt;%\n  mutate(cosine_distance = round(cosine_distance, 3)) %&gt;%\n  knitr::kable(col.names = c(\"Text 1\", \"Text 2\", \"Cosine Distance\", \"Genre 1\", \"Genre 2\"))\n\n\n\n\n\n\n\n\n\n\n\nText 1\nText 2\nCosine Distance\nGenre 1\nGenre 2\n\n\n\n\nGreat Expectations\nTom Sawyer\n0.622\nSocial Fiction\nAdventure\n\n\nFrankenstein\nPride & Prejudice\n0.703\nGothic\nSocial Fiction\n\n\nDracula\nFrankenstein\n0.849\nGothic\nGothic\n\n\nDracula\n20,000 Leagues\n0.865\nGothic\nScience Fiction\n\n\nFrankenstein\n20,000 Leagues\n0.880\nGothic\nScience Fiction\n\n\n\n\ncat(\"\\nLEAST SIMILAR TEXTS (Cosine Distance):\\n\")\n\n\nLEAST SIMILAR TEXTS (Cosine Distance):\n\ntail(cosine_pairs, 5) %&gt;%\n  select(text1_short, text2_short, cosine_distance, genre1, genre2) %&gt;%\n  mutate(cosine_distance = round(cosine_distance, 3)) %&gt;%\n  knitr::kable(col.names = c(\"Text 1\", \"Text 2\", \"Cosine Distance\", \"Genre 1\", \"Genre 2\"))\n\n\n\n\n\n\n\n\n\n\n\nText 1\nText 2\nCosine Distance\nGenre 1\nGenre 2\n\n\n\n\nAlice in Wonderland\nFrankenstein\n0.996\nFantasy\nGothic\n\n\nAlice in Wonderland\nLittle Women\n0.996\nFantasy\nDomestic Fiction\n\n\nAlice in Wonderland\nGreat Expectations\n0.997\nFantasy\nSocial Fiction\n\n\nAlice in Wonderland\nPride & Prejudice\n0.998\nFantasy\nSocial Fiction\n\n\nAlice in Wonderland\nTime Machine\n0.999\nFantasy\nScience Fiction",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#euclidean-distance",
    "href": "06-findingSimilarities.html#euclidean-distance",
    "title": "Finding Similarities between Texts",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\nEuclidean distance measures the straight-line distance between two points in space. Unlike cosine distance, it considers both the direction and magnitude of vectors, making it sensitive to the actual values of TF-IDF scores.\n\nMathematical Foundation\nFor two vectors a and b, Euclidean distance is:\n\\[\n\\text{Euclidean distance} = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n\\]\nWhere \\(n\\) is the number of dimensions (words in our vocabulary).\n\n# Calculate Euclidean distance matrix\neuclidean_dist_matrix &lt;- as.matrix(dist(tfidf_matrix, method = \"euclidean\"))\n\n# Display part of the Euclidean distance matrix\nround(euclidean_dist_matrix, 3) %&gt;%\n  knitr::kable(caption = \"Euclidean Distance Matrix (smaller = more similar)\")\n\n\nEuclidean Distance Matrix (smaller = more similar)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlice’s Adventures in Wonderland\nCranford\nDracula\nFrankenstein; Or, The Modern Prometheus\nGreat Expectations\nJane Eyre\nLittle Women\nPride and Prejudice\nThe Time Machine\nThe Wind in the Willows\nTom Sawyer\nTwenty Thousand Leagues Under the Seas\n\n\n\n\nAlice’s Adventures in Wonderland\n0.000\n0.097\n0.096\n0.096\n0.101\n0.097\n0.106\n0.098\n0.098\n0.105\n0.100\n0.097\n\n\nCranford\n0.097\n0.000\n0.019\n0.017\n0.036\n0.020\n0.049\n0.026\n0.027\n0.046\n0.033\n0.023\n\n\nDracula\n0.096\n0.019\n0.000\n0.014\n0.035\n0.019\n0.048\n0.026\n0.026\n0.045\n0.032\n0.021\n\n\nFrankenstein; Or, The Modern Prometheus\n0.096\n0.017\n0.014\n0.000\n0.034\n0.017\n0.047\n0.022\n0.024\n0.044\n0.031\n0.020\n\n\nGreat Expectations\n0.101\n0.036\n0.035\n0.034\n0.000\n0.036\n0.057\n0.040\n0.040\n0.054\n0.035\n0.037\n\n\nJane Eyre\n0.097\n0.020\n0.019\n0.017\n0.036\n0.000\n0.048\n0.027\n0.027\n0.046\n0.034\n0.023\n\n\nLittle Women\n0.106\n0.049\n0.048\n0.047\n0.057\n0.048\n0.000\n0.052\n0.052\n0.064\n0.055\n0.050\n\n\nPride and Prejudice\n0.098\n0.026\n0.026\n0.022\n0.040\n0.027\n0.052\n0.000\n0.032\n0.049\n0.038\n0.030\n\n\nThe Time Machine\n0.098\n0.027\n0.026\n0.024\n0.040\n0.027\n0.052\n0.032\n0.000\n0.049\n0.038\n0.029\n\n\nThe Wind in the Willows\n0.105\n0.046\n0.045\n0.044\n0.054\n0.046\n0.064\n0.049\n0.049\n0.000\n0.053\n0.047\n\n\nTom Sawyer\n0.100\n0.033\n0.032\n0.031\n0.035\n0.034\n0.055\n0.038\n0.038\n0.053\n0.000\n0.035\n\n\nTwenty Thousand Leagues Under the Seas\n0.097\n0.023\n0.021\n0.020\n0.037\n0.023\n0.050\n0.030\n0.029\n0.047\n0.035\n0.000\n\n\n\n\n\nLet’s visualize the Euclidean distances:\n\n# Create heatmap for Euclidean distances\neuclidean_dist_df &lt;- euclidean_dist_matrix %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"text1\") %&gt;%\n  pivot_longer(-text1, names_to = \"text2\", values_to = \"euclidean_distance\") %&gt;%\n  # Add genre information and short titles\n  left_join(similarity_texts %&gt;% select(title, genre) %&gt;% rename(text1 = title, genre1 = genre), by = \"text1\") %&gt;%\n  left_join(similarity_texts %&gt;% select(title, genre) %&gt;% rename(text2 = title, genre2 = genre), by = \"text2\") %&gt;%\n  mutate(\n    text1_short = case_when(\n      str_detect(text1, \"Pride\") ~ \"Pride & Prejudice\",\n      str_detect(text1, \"Frankenstein\") ~ \"Frankenstein\",\n      str_detect(text1, \"Alice\") ~ \"Alice in Wonderland\",\n      str_detect(text1, \"Twenty\") ~ \"20,000 Leagues\",\n      str_detect(text1, \"Treasure\") ~ \"Treasure Island\",\n      str_detect(text1, \"Dracula\") ~ \"Dracula\",\n      str_detect(text1, \"Time Machine\") ~ \"Time Machine\",\n      str_detect(text1, \"Wind\") ~ \"Wind in the Willows\",\n      str_detect(text1, \"Gatsby\") ~ \"Great Gatsby\",\n      str_detect(text1, \"Great Expectations\") ~ \"Great Expectations\",\n      TRUE ~ text1\n    ),\n    text2_short = case_when(\n      str_detect(text2, \"Pride\") ~ \"Pride & Prejudice\",\n      str_detect(text2, \"Frankenstein\") ~ \"Frankenstein\",\n      str_detect(text2, \"Alice\") ~ \"Alice in Wonderland\",\n      str_detect(text2, \"Twenty\") ~ \"20,000 Leagues\",\n      str_detect(text2, \"Treasure\") ~ \"Treasure Island\",\n      str_detect(text2, \"Dracula\") ~ \"Dracula\",\n      str_detect(text2, \"Time Machine\") ~ \"Time Machine\",\n      str_detect(text2, \"Wind\") ~ \"Wind in the Willows\",\n      str_detect(text2, \"Gatsby\") ~ \"Great Gatsby\",\n      str_detect(text2, \"Great Expectations\") ~ \"Great Expectations\",\n      TRUE ~ text2\n    )\n  )\n\nggplot(euclidean_dist_df, aes(x = text1_short, y = text2_short, fill = euclidean_distance)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", \n                       midpoint = median(euclidean_dist_df$euclidean_distance), \n                       name = \"Euclidean\\nDistance\") +\n  labs(\n    title = \"Euclidean Distance Between Texts\",\n    subtitle = \"Blue = More Similar, Red = More Different\",\n    x = \"Text 1\", y = \"Text 2\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n    axis.text.y = element_text(size = 9),\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\nLet’s find the most and least similar pairs using Euclidean distance:\n\n# Find most similar and least similar pairs using Euclidean distance\neuclidean_pairs &lt;- euclidean_dist_df %&gt;%\n  filter(text1 != text2) %&gt;%\n  filter(text1 &lt; text2) %&gt;%\n  arrange(euclidean_distance)\n\n# Most similar pairs\ncat(\"MOST SIMILAR TEXTS (Euclidean Distance):\\n\")\n\nMOST SIMILAR TEXTS (Euclidean Distance):\n\nhead(euclidean_pairs, 5) %&gt;%\n  select(text1_short, text2_short, euclidean_distance, genre1, genre2) %&gt;%\n  mutate(euclidean_distance = round(euclidean_distance, 3)) %&gt;%\n  knitr::kable(col.names = c(\"Text 1\", \"Text 2\", \"Euclidean Distance\", \"Genre 1\", \"Genre 2\"))\n\n\n\n\n\n\n\n\n\n\n\nText 1\nText 2\nEuclidean Distance\nGenre 1\nGenre 2\n\n\n\n\nDracula\nFrankenstein\n0.014\nGothic\nGothic\n\n\nCranford\nFrankenstein\n0.017\nDomestic Fiction\nGothic\n\n\nFrankenstein\nJane Eyre\n0.017\nGothic\nGothic\n\n\nCranford\nDracula\n0.019\nDomestic Fiction\nGothic\n\n\nDracula\nJane Eyre\n0.019\nGothic\nGothic\n\n\n\n\ncat(\"\\nLEAST SIMILAR TEXTS (Euclidean Distance):\\n\")\n\n\nLEAST SIMILAR TEXTS (Euclidean Distance):\n\ntail(euclidean_pairs, 5) %&gt;%\n  select(text1_short, text2_short, euclidean_distance, genre1, genre2) %&gt;%\n  mutate(euclidean_distance = round(euclidean_distance, 3)) %&gt;%\n  knitr::kable(col.names = c(\"Text 1\", \"Text 2\", \"Euclidean Distance\", \"Genre 1\", \"Genre 2\"))\n\n\n\n\n\n\n\n\n\n\n\nText 1\nText 2\nEuclidean Distance\nGenre 1\nGenre 2\n\n\n\n\nAlice in Wonderland\nTime Machine\n0.098\nFantasy\nScience Fiction\n\n\nAlice in Wonderland\nTom Sawyer\n0.100\nFantasy\nAdventure\n\n\nAlice in Wonderland\nGreat Expectations\n0.101\nFantasy\nSocial Fiction\n\n\nAlice in Wonderland\nWind in the Willows\n0.105\nFantasy\nFantasy\n\n\nAlice in Wonderland\nLittle Women\n0.106\nFantasy\nDomestic Fiction",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#comparing-cosine-vs.-euclidean-distance",
    "href": "06-findingSimilarities.html#comparing-cosine-vs.-euclidean-distance",
    "title": "Finding Similarities between Texts",
    "section": "Comparing Cosine vs. Euclidean Distance",
    "text": "Comparing Cosine vs. Euclidean Distance\nLet’s directly compare how these two distance measures rank text similarities:\n\n# Compare the two distance measures\ndistance_comparison &lt;- cosine_pairs %&gt;%\n  select(text1, text2, cosine_distance) %&gt;%\n  left_join(\n    euclidean_pairs %&gt;% select(text1, text2, euclidean_distance),\n    by = c(\"text1\", \"text2\")\n  ) %&gt;%\n  # Calculate rankings for each distance measure\n  mutate(\n    cosine_rank = rank(cosine_distance),\n    euclidean_rank = rank(euclidean_distance),\n    rank_difference = abs(cosine_rank - euclidean_rank)\n  ) %&gt;%\n  left_join(similarity_texts %&gt;% select(title, genre) %&gt;% rename(text1 = title, genre1 = genre), by = \"text1\") %&gt;%\n  left_join(similarity_texts %&gt;% select(title, genre) %&gt;% rename(text2 = title, genre2 = genre), by = \"text2\")\n\n# Show pairs where the rankings differ most\ncat(\"PAIRS WITH BIGGEST RANKING DIFFERENCES:\\n\")\n\nPAIRS WITH BIGGEST RANKING DIFFERENCES:\n\ndistance_comparison %&gt;%\n  arrange(desc(rank_difference)) %&gt;%\n  head(10) %&gt;%\n  mutate(\n    text1_short = str_trunc(text1, 20),\n    text2_short = str_trunc(text2, 20)\n  ) %&gt;%\n  select(text1_short, text2_short, cosine_rank, euclidean_rank, rank_difference, genre1, genre2) %&gt;%\n  knitr::kable(col.names = c(\"Text 1\", \"Text 2\", \"Cosine Rank\", \"Euclidean Rank\", \"Difference\", \"Genre 1\", \"Genre 2\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nText 1\nText 2\nCosine Rank\nEuclidean Rank\nDifference\nGenre 1\nGenre 2\n\n\n\n\nJane Eyre\nLittle Women\n9\n43\n34\nGothic\nDomestic Fiction\n\n\nLittle Women\nTom Sawyer\n23\n53\n30\nDomestic Fiction\nAdventure\n\n\nPride and Prejudice\nTwenty Thousand L…\n50\n20\n30\nSocial Fiction\nScience Fiction\n\n\nGreat Expectations\nTom Sawyer\n1\n28\n27\nSocial Fiction\nAdventure\n\n\nLittle Women\nTwenty Thousand L…\n24\n48\n24\nDomestic Fiction\nScience Fiction\n\n\nPride and Prejudice\nThe Time Machine\n47\n23\n24\nSocial Fiction\nScience Fiction\n\n\nDracula\nPride and Prejudice\n32\n13\n19\nGothic\nSocial Fiction\n\n\nThe Time Machine\nTwenty Thousand L…\n36\n19\n17\nScience Fiction\nScience Fiction\n\n\nDracula\nThe Time Machine\n30\n14\n16\nGothic\nScience Fiction\n\n\nGreat Expectations\nLittle Women\n39\n54\n15\nSocial Fiction\nDomestic Fiction\n\n\n\n\n\nLet’s create a scatter plot to visualize the relationship between the two distance measures:\n\n# Scatter plot comparing the two distance measures\ndistance_comparison %&gt;%\n  mutate(\n    same_genre = genre1 == genre2,\n    pair_label = ifelse(rank_difference &gt;= 10, \n                       paste(str_trunc(text1, 15), \"vs\", str_trunc(text2, 15)), \"\")\n  ) %&gt;%\n  ggplot(aes(x = cosine_distance, y = euclidean_distance, color = same_genre)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_text(aes(label = pair_label), hjust = 0, vjust = 0, size = 3, check_overlap = TRUE) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  labs(\n    title = \"Cosine vs. Euclidean Distance: How Do They Compare?\",\n    subtitle = \"Each point represents a pair of texts\",\n    x = \"Cosine Distance\",\n    y = \"Euclidean Distance\",\n    color = \"Same Genre\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"blue\"))\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#hierarchical-clustering-with-distance-measures",
    "href": "06-findingSimilarities.html#hierarchical-clustering-with-distance-measures",
    "title": "Finding Similarities between Texts",
    "section": "Hierarchical Clustering with Distance Measures",
    "text": "Hierarchical Clustering with Distance Measures\nOne powerful application of distance measures is hierarchical clustering - automatically grouping texts based on their similarities. Let’s create dendrograms using both distance measures:\n\n# Create hierarchical clustering dendrograms\npar(mfrow = c(2, 1))\n\n# Cosine distance clustering\ncosine_hclust &lt;- hclust(as.dist(cosine_dist_matrix), method = \"ward.D2\")\nplot(cosine_hclust, main = \"Hierarchical Clustering - Cosine Distance\", \n     xlab = \"Texts\", ylab = \"Distance\", cex = 0.8)\n\n# Euclidean distance clustering  \neuclidean_hclust &lt;- hclust(as.dist(euclidean_dist_matrix), method = \"ward.D2\")\nplot(euclidean_hclust, main = \"Hierarchical Clustering - Euclidean Distance\", \n     xlab = \"Texts\", ylab = \"Distance\", cex = 0.8)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nLet’s examine how well these clusters align with our known genres:\n\n# Create clusters and compare with actual genres\n# Cut the dendrograms to create groups (6 genres in our focused subset)\ncosine_clusters &lt;- cutree(cosine_hclust, k = 6)  \neuclidean_clusters &lt;- cutree(euclidean_hclust, k = 6)\n\n# Create comparison table\ncluster_comparison &lt;- data.frame(\n  Title = names(cosine_clusters),\n  Actual_Genre = similarity_texts$genre[match(names(cosine_clusters), similarity_texts$title)],\n  Cosine_Cluster = cosine_clusters,\n  Euclidean_Cluster = euclidean_clusters\n) %&gt;%\n  mutate(\n    Title_Short = case_when(\n      str_detect(Title, \"Pride\") ~ \"Pride & Prejudice\",\n      str_detect(Title, \"Frankenstein\") ~ \"Frankenstein\",\n      str_detect(Title, \"Alice\") ~ \"Alice in Wonderland\",\n      str_detect(Title, \"Twenty\") ~ \"20,000 Leagues\",\n      str_detect(Title, \"Treasure\") ~ \"Treasure Island\",\n      str_detect(Title, \"Dracula\") ~ \"Dracula\",\n      str_detect(Title, \"Time Machine\") ~ \"Time Machine\",\n      str_detect(Title, \"Wind\") ~ \"Wind in the Willows\",\n      str_detect(Title, \"Gatsby\") ~ \"Great Gatsby\",\n      str_detect(Title, \"Great Expectations\") ~ \"Great Expectations\",\n      TRUE ~ Title\n    )\n  ) %&gt;%\n  arrange(Actual_Genre, Title_Short)\n\nknitr::kable(cluster_comparison %&gt;% select(-Title), \n             col.names = c(\"Book\", \"Actual Genre\", \"Cosine Cluster\", \"Euclidean Cluster\"),\n             caption = \"Clustering Results vs. Actual Genres\")\n\n\nClustering Results vs. Actual Genres\n\n\n\n\n\n\n\n\n\n\nBook\nActual Genre\nCosine Cluster\nEuclidean Cluster\n\n\n\n\nTom Sawyer\nAdventure\n5\n6\nTom Sawyer\n\n\nCranford\nDomestic Fiction\n2\n2\nCranford\n\n\nLittle Women\nDomestic Fiction\n2\n4\nLittle Women\n\n\nAlice’s Adventures in Wonderland\nFantasy\n1\n1\nAlice in Wonderland\n\n\nThe Wind in the Willows\nFantasy\n6\n5\nWind in the Willows\n\n\nDracula\nGothic\n3\n2\nDracula\n\n\nFrankenstein; Or, The Modern Prometheus\nGothic\n4\n2\nFrankenstein\n\n\nJane Eyre\nGothic\n2\n2\nJane Eyre\n\n\nTwenty Thousand Leagues Under the Seas\nScience Fiction\n3\n2\n20,000 Leagues\n\n\nThe Time Machine\nScience Fiction\n6\n2\nTime Machine\n\n\nGreat Expectations\nSocial Fiction\n5\n3\nGreat Expectations\n\n\nPride and Prejudice\nSocial Fiction\n4\n2\nPride & Prejudice",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#genre-similarity-analysis",
    "href": "06-findingSimilarities.html#genre-similarity-analysis",
    "title": "Finding Similarities between Texts",
    "section": "Genre Similarity Analysis",
    "text": "Genre Similarity Analysis\nFinally, let’s examine how our distance measures capture similarities within and between literary genres:\n\n# Calculate average distances within and between genres\ngenre_distances &lt;- cosine_dist_df %&gt;%\n  filter(text1 != text2) %&gt;%\n  # Add euclidean distance data\n  left_join(euclidean_dist_df %&gt;% select(text1, text2, euclidean_distance), \n            by = c(\"text1\", \"text2\")) %&gt;%\n  mutate(\n    relationship = case_when(\n      genre1 == genre2 ~ paste(\"Within\", genre1),\n      TRUE ~ paste(pmin(genre1, genre2), \"vs\", pmax(genre1, genre2))\n    )\n  ) %&gt;%\n  group_by(relationship) %&gt;%\n  summarise(\n    avg_cosine_distance = mean(cosine_distance),\n    avg_euclidean_distance = mean(euclidean_distance, na.rm = TRUE),\n    n_pairs = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(avg_cosine_distance)\n\n# Visualize genre relationships\ngenre_distances %&gt;%\n  pivot_longer(cols = c(avg_cosine_distance, avg_euclidean_distance), \n               names_to = \"distance_type\", values_to = \"distance\") %&gt;%\n  mutate(\n    distance_type = case_when(\n      distance_type == \"avg_cosine_distance\" ~ \"Cosine Distance\",\n      distance_type == \"avg_euclidean_distance\" ~ \"Euclidean Distance\"\n    ),\n    within_genre = str_detect(relationship, \"Within\")\n  ) %&gt;%\n  ggplot(aes(x = reorder(relationship, distance), y = distance, \n             fill = within_genre, alpha = distance_type)) +\n  geom_col(position = \"dodge\") +\n  scale_alpha_manual(values = c(\"Cosine Distance\" = 0.8, \"Euclidean Distance\" = 0.6)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"darkred\")) +\n  coord_flip() +\n  labs(\n    title = \"Average Distances Within and Between Genres\",\n    subtitle = \"Do texts cluster by literary genre?\",\n    x = \"Genre Relationship\",\n    y = \"Average Distance\",\n    fill = \"Within Genre\",\n    alpha = \"Distance Type\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "06-findingSimilarities.html#summary-and-interpretation",
    "href": "06-findingSimilarities.html#summary-and-interpretation",
    "title": "Finding Similarities between Texts",
    "section": "Summary and Interpretation",
    "text": "Summary and Interpretation\n\n# Summary statistics\ncat(\"SUMMARY OF FINDINGS:\\n\")\n\nSUMMARY OF FINDINGS:\n\ncat(\"====================\\n\\n\")\n\n====================\n\n# Overall correlation between distance measures\ncorrelation &lt;- cor(distance_comparison$cosine_distance, distance_comparison$euclidean_distance)\ncat(\"Correlation between Cosine and Euclidean distances:\", round(correlation, 3), \"\\n\\n\")\n\nCorrelation between Cosine and Euclidean distances: 0.401 \n\n# Within-genre vs between-genre similarities\nwithin_genre_avg_cosine &lt;- mean(genre_distances$avg_cosine_distance[str_detect(genre_distances$relationship, \"Within\")])\nbetween_genre_avg_cosine &lt;- mean(genre_distances$avg_cosine_distance[!str_detect(genre_distances$relationship, \"Within\")])\n\ncat(\"Average Cosine Distance:\\n\")\n\nAverage Cosine Distance:\n\ncat(\"  Within genres:\", round(within_genre_avg_cosine, 3), \"\\n\")\n\n  Within genres: 0.969 \n\ncat(\"  Between genres:\", round(between_genre_avg_cosine, 3), \"\\n\")\n\n  Between genres: 0.96 \n\ncat(\"  Difference:\", round(between_genre_avg_cosine - within_genre_avg_cosine, 3), \"\\n\\n\")\n\n  Difference: -0.009 \n\n# Most distinctive genres (smallest within-genre distances)\nmost_cohesive_genre &lt;- genre_distances %&gt;%\n  filter(str_detect(relationship, \"Within\")) %&gt;%\n  slice_min(avg_cosine_distance, n = 1)\n\ncat(\"Most cohesive genre (by cosine distance):\", most_cohesive_genre$relationship, \n    \"with average distance\", round(most_cohesive_genre$avg_cosine_distance, 3), \"\\n\")\n\nMost cohesive genre (by cosine distance): Within Gothic with average distance 0.908 \n\n\n\n\n\n\n\n\nKey Insights from Distance Analysis\n\n\n\nOur exploration of text similarity reveals several important patterns:\n1. Distance Measure Differences: - Cosine distance focuses on proportional word usage patterns, making it excellent for identifying thematic similarities regardless of text length - Euclidean distance considers both proportions and magnitudes, making it sensitive to vocabulary intensity and text scale\n2. Genre Clustering: - Texts within the same genre generally show smaller distances (higher similarity) than texts from different genres - However, some cross-genre similarities emerge, suggesting shared narrative techniques or thematic elements\n3. Mathematical vs. Literary Categories: - While distance measures often align with literary genres, they sometimes reveal unexpected similarities based on vocabulary patterns - This demonstrates how computational methods can both confirm and challenge traditional literary classifications\n4. Applications: - Authorship attribution: Comparing unknown texts to known authors - Genre classification: Automatically categorizing texts by similarity to genre exemplars\n- Literary influence: Detecting vocabulary patterns that suggest textual relationships - Corpus exploration: Discovering unexpected connections in large text collections\n\n\nThese similarity measures provide the foundation for more advanced techniques like clustering, topic modeling, and machine learning applications in literary analysis. In our next episode, we’ll explore how these concepts extend into topic modeling and latent semantic analysis.",
    "crumbs": [
      "6. Finding Similarities"
    ]
  },
  {
    "objectID": "05-TF-IDF.html",
    "href": "05-TF-IDF.html",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "",
    "text": "In the previous chapter, we explored basic text analysis techniques like word frequencies and lexical density. While these methods help us understand general patterns in our texts, they don’t tell us which words are truly distinctive or characteristic of specific authors, genres, or time periods.\nThis is where TF-IDF (Term Frequency-Inverse Document Frequency) becomes invaluable. TF-IDF helps us identify words that are not only frequent in a particular document, but also relatively rare across the entire corpus. This makes it an excellent tool for discovering what makes each text unique.",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "05-TF-IDF.html#what-is-tf-idf",
    "href": "05-TF-IDF.html#what-is-tf-idf",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "What is TF-IDF?",
    "text": "What is TF-IDF?\nTF-IDF combines two important concepts:\n\nTerm Frequency (TF): How often a word appears in a specific document\nInverse Document Frequency (IDF): How rare the word is across all documents in the corpus\n\nThe mathematical formula is:\n\\[\n\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)\n\\]\nWhere: - \\(\\text{TF}(t,d)\\) = frequency of term \\(t\\) in document \\(d\\) - \\(\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total documents}}{\\text{Documents containing } t}\\right)\\)\n\nUnderstanding the Components\nTerm Frequency (TF) can be calculated in several ways: - Raw count: Simply how many times the word appears - Relative frequency: Raw count divided by total words in the document - Log normalization: \\(1 + \\log(\\text{raw count})\\) (used to dampen the effect of very high frequencies)\nInverse Document Frequency (IDF) gives higher weights to words that appear in fewer documents: - If a word appears in all documents, IDF approaches 0 - If a word appears in only one document, IDF is at its maximum - The logarithm smooths this relationship\nLet’s explore this with our corpus!",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "05-TF-IDF.html#setting-up-our-analysis",
    "href": "05-TF-IDF.html#setting-up-our-analysis",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "Setting Up Our Analysis",
    "text": "Setting Up Our Analysis\nLet’s work with our full corpus from the previous chapter, treating each book as a separate document for TF-IDF analysis. We’ll handle possessive forms by removing the ’s suffix during tokenization:\n\n# Create our full corpus (25 classic works)\ncorpus &lt;- tribble(\n  ~author, ~title, ~year, ~glID, ~genre,\n  \"Jane Austen\", \"Pride and Prejudice\", 1813, 1342, \"Social Fiction\",\n  \"Charles Dickens\", \"A Tale of Two Cities\", 1859, 98, \"Social Fiction\",\n  \"F. Scott Fitzgerald\", \"The Great Gatsby\", 1925, 64317, \"Social Fiction\",\n  \"Mary Wollstonecraft Shelley\", \"Frankenstein; Or, The Modern Prometheus\", 1818, 84, \"Gothic\",\n  \"Herman Melville\", \"Moby Dick\", 1851, 2701, \"Adventure\",\n  \"Louisa May Alcott\", \"Little Women\", 1868, 37106, \"Domestic Fiction\",\n  \"Mark Twain\", \"Tom Sawyer\", 1876, 74, \"Adventure\",\n  \"Jonathan Swift\", \"Gulliver's Travels\", 1726, 17157, \"Satirical Fiction\",\n  \"E. M. Forster\", \"A Room with a View\", 1908, 2641, \"Social Fiction\",\n  \"Elizabeth Von Arnim\", \"The Enchanted April\", 1922, 16389, \"Social Fiction\",\n  \"Lewis Carroll\", \"Alice's Adventures in Wonderland\", 1865, 11, \"Fantasy\",\n  \"Elizabeth Gaskell\", \"Cranford\", 1853, 394, \"Domestic Fiction\",\n  \"Charles Dickens\", \"The Pickwick Papers\", 1836, 580, \"Social Fiction\",\n  \"J. M. Barrie\", \"Peter Pan\", 1911, 16, \"Fantasy\",\n  \"Charles Dickens\", \"Great Expectations\", 1861, 1400, \"Social Fiction\",\n  \"Robert Louis Stevenson\", \"Treasure Island\", 1883, 120, \"Adventure\",\n  \"Kenneth Grahame\", \"The Wind in the Willows\", 1908, 27805, \"Fantasy\",\n  \"Jules Verne\", \"Twenty Thousand Leagues Under the Seas\", 1870, 2488, \"Science Fiction\",\n  \"Jules Verne\", \"A Journey to the Centre of the Earth\", 1864, 18857, \"Science Fiction\",\n  \"Jules Verne\", \"Around the World in Eighty Days\", 1873, 103, \"Adventure\",\n  \"Bram Stoker\", \"Dracula\", 1897, 521, \"Gothic\",\n  \"H. G. Wells\", \"The Time Machine\", 1895, 43, \"Science Fiction\",\n  \"Charlotte Brontë\", \"Jane Eyre\", 1847, 145, \"Gothic\",\n  \"Jane Austen\", \"Northanger Abbey\", 1817, 829, \"Social Fiction\",\n  \"Elizabeth Gaskell\", \"North and South\", 1855, 203, \"Social Fiction\"\n)\n\n# Download and clean the texts\ncorpus_texts &lt;- corpus %&gt;%\n   mutate(text = map(glID, ~gutenberg_download(.x) %&gt;%\n   pull(text) %&gt;%\n   paste(collapse = \" \"))) %&gt;%\n   mutate(\n    text_clean = str_to_lower(text) %&gt;%\n       str_remove_all(\"'s\\\\b\") %&gt;%                    # Remove possessive 's from full text\n       str_remove_all(\"[[:punct:]]\") %&gt;%              # Remove remaining punctuation\n       str_squish()                                    # Remove extra whitespace\n   ) %&gt;%\n   select(author, title, year, genre, text_clean)  # Keep all metadata columns\n\nDetermining mirror for Project Gutenberg from\nhttps://www.gutenberg.org/robot/harvest.\nUsing mirror http://aleph.gutenberg.org.\n\ncorpus_texts\n\n# A tibble: 25 × 5\n   author                      title                       year genre text_clean\n   &lt;chr&gt;                       &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     \n 1 Jane Austen                 Pride and Prejudice         1813 Soci… george al…\n 2 Charles Dickens             A Tale of Two Cities        1859 Soci… a tale of…\n 3 F. Scott Fitzgerald         The Great Gatsby            1925 Soci… the great…\n 4 Mary Wollstonecraft Shelley Frankenstein; Or, The Mod…  1818 Goth… frankenst…\n 5 Herman Melville             Moby Dick                   1851 Adve… mobydick …\n 6 Louisa May Alcott           Little Women                1868 Dome… illustrat…\n 7 Mark Twain                  Tom Sawyer                  1876 Adve… the adven…\n 8 Jonathan Swift              Gulliver's Travels          1726 Sati… gulliver …\n 9 E. M. Forster               A Room with a View          1908 Soci… illustrat…\n10 Elizabeth Von Arnim         The Enchanted April         1922 Soci… illustrat…\n# ℹ 15 more rows",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "05-TF-IDF.html#calculating-tf-idf",
    "href": "05-TF-IDF.html#calculating-tf-idf",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "Calculating TF-IDF",
    "text": "Calculating TF-IDF\nThe tidytext package makes calculating TF-IDF straightforward with the bind_tf_idf() function. Let’s start by tokenizing our corpus and preparing it for TF-IDF analysis.\nTo improve our thematic analysis, we’ll also apply lemmatization - a process that reduces words to their base forms (e.g., “whales” → “whale”, “running” → “run”). This helps us treat related word forms as the same concept, providing cleaner thematic patterns:\n\n# Tokenize and prepare for TF-IDF with lemmatization\ncorpus_words &lt;- corpus_texts %&gt;%\n  select(title, author, genre, text_clean) %&gt;%\n  unnest_tokens(word, text_clean) %&gt;%\n  # Apply lemmatization to reduce words to their base forms\n  mutate(word = lemmatize_words(word)) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  count(title, word, sort = TRUE)\n\n# Display the structure\ncorpus_words %&gt;%\n  head(10)\n\n# A tibble: 10 × 3\n   title               word          n\n   &lt;chr&gt;               &lt;chr&gt;     &lt;int&gt;\n 1 The Pickwick Papers pickwick   2346\n 2 The Pickwick Papers sir        1474\n 3 Little Women        jo         1412\n 4 Moby Dick           whale      1359\n 5 The Pickwick Papers sam        1194\n 6 The Pickwick Papers reply      1097\n 7 The Pickwick Papers weller      987\n 8 The Pickwick Papers gentleman   983\n 9 Cranford            miss        869\n10 Jane Eyre           lydgate     858\n\n\nNow let’s calculate TF-IDF scores:\n\n# Calculate TF-IDF\ncorpus_tfidf &lt;- corpus_words %&gt;%\n  bind_tf_idf(word, title, n)\n\n# Display the words with highest TF-IDF scores\ncorpus_tfidf %&gt;%\n  arrange(desc(tf_idf)) %&gt;%\n  head(20)\n\n# A tibble: 20 × 6\n   title                            word             n      tf   idf tf_idf\n   &lt;chr&gt;                            &lt;chr&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alice's Adventures in Wonderland alice          770 0.0489   2.53 0.124 \n 2 Peter Pan                        wendy          332 0.0223   3.22 0.0717\n 3 Around the World in Eighty Days  fogg           601 0.0243   2.53 0.0615\n 4 Around the World in Eighty Days  passepartout   402 0.0163   3.22 0.0524\n 5 The Time Machine                 utterson       127 0.0152   3.22 0.0490\n 6 The Enchanted April              wilkins        457 0.0193   2.53 0.0487\n 7 Cranford                         matty          345 0.0151   3.22 0.0487\n 8 Little Women                     jo            1412 0.0212   2.12 0.0450\n 9 The Pickwick Papers              pickwick      2346 0.0208   2.12 0.0440\n10 The Great Gatsby                 gatsby         190 0.0111   3.22 0.0356\n11 Little Women                     meg            701 0.0105   3.22 0.0339\n12 The Enchanted April              fisher         373 0.0158   2.12 0.0334\n13 Around the World in Eighty Days  phileas        255 0.0103   3.22 0.0332\n14 The Enchanted April              arbuthnot      239 0.0101   3.22 0.0325\n15 Pride and Prejudice              darcy          380 0.00990  3.22 0.0319\n16 The Time Machine                 jekyll          82 0.00982  3.22 0.0316\n17 A Room with a View               lucy           437 0.0195   1.61 0.0314\n18 Little Women                     laurie         613 0.00922  3.22 0.0297\n19 Jane Eyre                        lydgate        858 0.00860  3.22 0.0277\n20 The Wind in the Willows          toad           444 0.0217   1.27 0.0277\n\n\nLet’s examine what these high TF-IDF words tell us about each book:\n\n# Top TF-IDF words for each book\ntop_tfidf &lt;- corpus_tfidf %&gt;%\n  group_by(title) %&gt;%\n  slice_max(tf_idf, n = 10) %&gt;%\n  ungroup()\n\ntop_tfidf\n\n# A tibble: 251 × 6\n   title                                word           n      tf   idf  tf_idf\n   &lt;chr&gt;                                &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 A Journey to the Centre of the Earth hans         168 0.00545 2.53  0.0138 \n 2 A Journey to the Centre of the Earth uncle        502 0.0163  0.446 0.00727\n 3 A Journey to the Centre of the Earth sneffels      49 0.00159 3.22  0.00512\n 4 A Journey to the Centre of the Earth professor    184 0.00597 0.734 0.00438\n 5 A Journey to the Centre of the Earth hardwigg      41 0.00133 3.22  0.00428\n 6 A Journey to the Centre of the Earth saknussemm    41 0.00133 3.22  0.00428\n 7 A Journey to the Centre of the Earth iceland       48 0.00156 2.53  0.00393\n 8 A Journey to the Centre of the Earth lava          49 0.00159 2.12  0.00337\n 9 A Journey to the Centre of the Earth granite       53 0.00172 1.83  0.00315\n10 A Journey to the Centre of the Earth raft          88 0.00285 1.02  0.00292\n# ℹ 241 more rows\n\n\n\nUnderstanding the Results\nLet’s examine what these distinctive words reveal:\n\n# Display top words by book in a more readable format\ntop_tfidf %&gt;%\n  select(title, word, tf_idf) %&gt;%\n  mutate(tf_idf = round(tf_idf, 4)) %&gt;%\n  group_by(title) %&gt;%\n  summarise(\n    top_words = paste(word, collapse = \", \"),\n    .groups = \"drop\"\n  )\n\n# A tibble: 25 × 2\n   title                                   top_words                            \n   &lt;chr&gt;                                   &lt;chr&gt;                                \n 1 A Journey to the Centre of the Earth    hans, uncle, sneffels, professor, ha…\n 2 A Room with a View                      lucy, bartlett, cecil, honeychurch, …\n 3 A Tale of Two Cities                    lorry, defarge, carton, manette, pro…\n 4 Alice's Adventures in Wonderland        alice, gryphon, hatter, dormouse, du…\n 5 Around the World in Eighty Days         fogg, passepartout, phileas, aouda, …\n 6 Cranford                                matty, cranford, jenkyns, jamieson, …\n 7 Dracula                                 xury, friday, canoe, brazils, island…\n 8 Frankenstein; Or, The Modern Prometheus clerval, justine, felix, frankenstei…\n 9 Great Expectations                      joe, wemmick, havisham, estella, her…\n10 Gulliver's Travels                      glumdalclitch, blefuscu, majesty, em…\n# ℹ 15 more rows\n\n\nNotice how the top TF-IDF words immediately reveal: - Character names: “alice” (Alice’s Adventures in Wonderland), “fogg” and “passepartout” (Around the World in Eighty Days), “matty” (Cranford) - Setting-specific terms: “sneffels” (A Journey to the Centre of the Earth), “blefuscu” (Gulliver’s Travels), “brazils” (Adventure novels) - Genre-specific vocabulary: Scientific and technical terms in Jules Verne works, fantastical locations in Swift’s satire",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "05-TF-IDF.html#visualizing-tf-idf",
    "href": "05-TF-IDF.html#visualizing-tf-idf",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "Visualizing TF-IDF",
    "text": "Visualizing TF-IDF\nRather than standard bar charts, let’s create visualizations that reveal the literary nature of our corpus and help us understand how TF-IDF captures the essence of different works.\n\nAuthors’ Distinctive Vocabularies\nLet’s examine authors who have multiple works in our corpus to see their consistent vocabulary patterns:\n\n# Focus on authors with multiple works\nmulti_work_authors &lt;- corpus %&gt;%\n  count(author, sort = TRUE) %&gt;%\n  filter(n &gt; 1) %&gt;%\n  pull(author)\n\n# Get top TF-IDF words for these authors' works\nauthor_vocabulary &lt;- corpus_tfidf %&gt;%\n  left_join(corpus %&gt;% select(title, author), by = \"title\") %&gt;%\n  filter(author %in% multi_work_authors) %&gt;%\n  group_by(author, title) %&gt;%\n  slice_max(tf_idf, n = 6) %&gt;%\n  ungroup() %&gt;%\n  # Create a combined identifier for better plotting\n  mutate(work_id = paste(str_wrap(author, 15), \"-\", str_trunc(title, 20)))\n\nauthor_vocabulary %&gt;%\n  ggplot(aes(x = reorder_within(word, tf_idf, work_id), y = tf_idf, fill = author)) +\n  geom_col() +\n  scale_x_reordered() +\n  coord_flip() +\n  facet_wrap(~work_id, scales = \"free\", ncol = 2) +\n  labs(\n    title = \"Authors' Distinctive Words Across Multiple Works\",\n    subtitle = \"Do authors maintain consistent vocabulary patterns?\",\n    x = \"Distinctive Words\",\n    y = \"TF-IDF Score\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 9),\n    axis.text = element_text(size = 8)\n  ) +\n  guides(fill = guide_legend(title = \"Author\"))\n\n\n\n\n\n\n\n\n\n\nTop TF-IDF Terms by Book\nBefore diving into systematic analysis, let’s create a simple reference list of the top 10 TF-IDF terms for each book:\n\n# Create a clean reference list of top TF-IDF terms\ntop_terms_by_book &lt;- corpus_tfidf %&gt;%\n  group_by(title) %&gt;%\n  slice_max(tf_idf, n = 10) %&gt;%\n  summarise(\n    top_10_terms = paste(word, collapse = \", \"),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(title)\n\n# Display as a clean table\ntop_terms_by_book %&gt;%\n  mutate(\n    title = str_trunc(title, 35),\n    top_10_terms = str_wrap(top_10_terms, 60)\n  ) %&gt;%\n  knitr::kable(\n    col.names = c(\"Book Title\", \"Top 10 TF-IDF Terms\"),\n    caption = \"Most distinctive words for each book in our corpus\"\n  )\n\n\nMost distinctive words for each book in our corpus\n\n\n\n\n\n\nBook Title\nTop 10 TF-IDF Terms\n\n\n\n\nA Journey to the Centre of the E…\nhans, uncle, sneffels, professor, hardwigg, saknussemm,\n\n\niceland, lava, granite, raft\n\n\n\nA Room with a View\nlucy, bartlett, cecil, honeychurch, beebe, freddy, emerson,\n\n\ncharlotte, vyse, florence\n\n\n\nA Tale of Two Cities\nlorry, defarge, carton, manette, pross, darnay, lucie,\n\n\ncruncher, monseigneur, stryver\n\n\n\nAlice’s Adventures in Wonderland\nalice, gryphon, hatter, dormouse, duchess, turtle,\n\n\ncaterpillar, alices, rabbit, mouse\n\n\n\nAround the World in Eighty Days\nfogg, passepartout, phileas, aouda, hong, kong, detective,\n\n\nyokohama, bombay, foggs\n\n\n\nCranford\nmatty, cranford, jenkyns, jamieson, glenmire, forrester,\n\n\nmattys, hoggins, deborah, jessie\n\n\n\nDracula\nxury, friday, canoe, brazils, island, ship, boat,\n\n\ndeliverance, shore, rice\n\n\n\nFrankenstein; Or, The Modern Pro…\nclerval, justine, felix, frankenstein, safie, elizabeth,\n\n\ngeneva, agatha, dæmon, ingolstadt\n\n\n\nGreat Expectations\njoe, wemmick, havisham, estella, herbert, pip, jaggers,\n\n\nbiddy, pumblechook, wopsle\n\n\n\nGulliver’s Travels\nglumdalclitch, blefuscu, majesty, emperor, lilliput, cent,\n\n\nedit, manmountain, honor, imperial\n\n\n\nJane Eyre\nlydgate, dorothea, casaubon, bulstrode, rosamond, fred,\n\n\ngarth, brooke, middlemarch, celia\n\n\n\nLittle Women\njo, meg, laurie, amy, beth, laurence, hannah, bhaer, brooke,\n\n\nteddy\n\n\n\nMoby Dick\nwhale, ahab, stubb, queequeg, sperm, starbuck, pequod,\n\n\nnantucket, leviathan, harpoon\n\n\n\nNorth and South\neva, clare, legree, ophelia, haley, masr, cassy, shelby,\n\n\ntom, yer\n\n\n\nNorthanger Abbey\nyahoos, houyhnhnms, yahoo, glumdalclitch, majestys,\n\n\nblefuscu, majesty, emperor, houyhnhnm, luggnagg\n\n\n\nPeter Pan\nwendy, peter, nana, smee, michael, tootle, redskin, nib,\n\n\ntink, wendys\n\n\n\nPride and Prejudice\ndarcy, bennet, bingley, elizabeth, wickham, lydia, collins,\n\n\njane, longbourn, netherfield\n\n\n\nThe Enchanted April\nwilkins, fisher, arbuthnot, caroline, briggs, lotty,\n\n\nsalvatore, mellersh, frederick, fishers, francesca\n\n\n\nThe Great Gatsby\ngatsby, daisy, gatsbys, wilson, daisys, car, tom, jordan,\n\n\ngarage, wolfshiem\n\n\n\nThe Pickwick Papers\npickwick, weller, sam, winkle, wardle, tupman, perker, wery,\n\n\npott, bardell\n\n\n\nThe Time Machine\nutterson, jekyll, hyde, poole, lanyon, lawyer, jekylls,\n\n\nenfield, dr, edward\n\n\n\nThe Wind in the Willows\ntoad, mole, badger, rat, ratty, otter, motorcar, bargewoman,\n\n\ngipsy, stoat\n\n\n\nTom Sawyer\nhuck, tom, becky, sid, injun, joe, thatcher, aint, harper,\n\n\npolly\n\n\n\nTreasure Island\nlivesey, jim, hispaniola, capn, squire, hawkins, smollett,\n\n\ngunn, trelawney, stockade\n\n\n\nTwenty Thousand Leagues Under th…\nconseil, nemo, nautilus, ned, meter, canadian, nautiluss,\n\n\nunderwater, aronnax, whale\n\n\n\n\n\n\nThe table above clearly shows that character names dominate TF-IDF results in literature. Looking at the terms, we can manually identify character names to separate them from thematic words for deeper analysis.\n\n\nThematic Words\nNow let’s focus on the distinctive words that aren’t character names, to discover the thematic fingerprint of each work. We’ll create a comprehensive list of character names to exclude from our analysis:\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis list was created manually by identifying character names in the text that appeared as results in the table above and in the subsequent thematic fingerprint results. The process was repeated until we achieved this comprehensive list.\n\n\n\n\n# Comprehensive list of character names\ncharacter_names &lt;- c(\"darcy\", \"bennet\", \"bingley\", \"elizabeth\", \"wickham\", \"lydia\", \"collins\", \"jane\", \"alice\", \"gryphon\", \"hatter\", \"turtle\", \"dormouse\", \"duchess\", \"caterpillar\", \"rabbit\", \"mouse\", \"francis\", \"huckleberry\", \"sawyer\", \"potter\", \"joe\", \"ben\", \"cranford\", \"martha\", \"jamieson\", \"matilda\", \"teddy\", \"beth\", \"meg\", \"demi\", \"laurie\", \"gatsby\", \"daisy\", \"tom\", \"jordan\", \"wilson\", \"wolfshiem\", \"hans\", \"lucy\", \"bartlett\", \"cecil\", \"honeychurch\", \"beebe\", \"freddy\", \"emerson\", \"vyse\", \"charlotte\", \"lorry\", \"defarge\", \"manette\", \"pross\", \"darnay\", \"lucie\", \"carton\", \"cruncher\", \"stryver\", \"fogg\", \"passepartout\", \"phileas\", \"aouda\", \"foggs\", \"matty\", \"jenkyns\", \"glenmire\", \"forrester\", \"mattys\", \"hoggins\", \"deborah\", \"jessie\", \"helsing\", \"mina\", \"jonathan\", \"harker\", \"van\", \"godalming\", \"seward\", \"quincey\", \"harkers\", \"clerval\", \"justine\", \"felix\", \"frankenstein\", \"safie\", \"agatha\", \"wemmick\", \"havisham\", \"estella\", \"biddy\", \"jaggers\", \"herbert\", \"pip\", \"pumblechook\", \"wopsle\", \"glumdalclitch\", \"rochester\", \"adèle\", \"eyre\", \"fairfax\", \"bessie\", \"ingram\", \"reed\", \"jo\", \"amy\", \"jos\", \"laurence\", \"brooke\", \"bhaer\", \"hannah\", \"ahab\", \"stubb\", \"queequeg\", \"starbuck\", \"ahabs\", \"margaret\", \"thornton\", \"hale\", \"dixon\", \"margarets\", \"edith\", \"higgins\", \"lennox\", \"catherine\", \"tilney\", \"morland\", \"thorpe\", \"isabella\", \"allen\", \"eleanor\", \"catherines\", \"tilneys\", \"thorpes\", \"wendy\", \"peter\", \"nana\", \"michael\", \"smee\", \"tootles\", \"nibs\", \"tink\", \"wendys\", \"starkey\", \"wilkins\", \"fisher\", \"arbuthnot\", \"caroline\", \"briggs\", \"lotty\", \"salvatore\", \"mellersh\", \"frederick\", \"scrap\", \"pickwick\", \"weller\", \"winkle\", \"sam\", \"wardle\", \"tupman\", \"pickwicks\", \"perker\", \"snodgrass\", \"pott\", \"weena\", \"psychologist\", \"filby\", \"toad\", \"mole\", \"badger\", \"rat\", \"ratty\", \"toads\", \"huck\", \"becky\", \"sid\", \"injun\", \"thatcher\", \"toms\", \"harper\", \"jim\", \"livesey\", \"hawkins\", \"trelawney\", \"conseil\", \"nemo\", \"ned\", \"aronnax\", \"eva\", \"clare\", \"legree\", \"ophelia\", \"haley\", \"cassy\", \"shelby\", \"topsy\", \"morgan\", \"diana\", \"helen\", \"mason\", \"saknussemm\", \"hardwigg\", \"emersons\", \"alans\", \"minnie\", \"lucys\", \"bertolini\", \"cecils\", \"jerry\", \"tellsons\", \"jacques\", \"marquis\", \"antoine\", \"havishams\", \"drummle\", \"handel\", \"orlick\", \"compeyson\", \"joes\", \"gargery\", \"milton\", \"helstone\", \"bessy\", \"thorntons\", \"hales\", \"shaw\", \"boucher\", \"northanger\", \"fullerton\", \"morlands\", \"allens\", \"henry\", \"woodston\", \"isabellas\", \"francesca\", \"meryton\", \"pemberley\", \"bingleys\", \"domenico\", \"wilkinss\", \"kate\", \"costanza\", \"gatsbys\", \"daisys\", \"buchanan\", \"michaelis\", \"mckee\", \"bardell\", \"sammy\", \"dodson\", \"bob\", \"arabella\", \"masr\", \"chloe\", \"marie\", \"eliza\", \"andy\", \"emmeline\", \"polly\", \"hucks\", \"alices\", \"jamiesons\", \"sallie\", \"marmee\", \"fred\", \"hook\", \"sewards\", \"morris\", \"arthur\", \"renfield\", \"victor\", \"rochesters\", \"georgiana\", \"gretchen\", \"alan\", \"cissie\", \"sydney\", \"fanny\", \"kitty\", \"lizzy\", \"james\", \"henrys\", \"myrtle\", \"baker\", \"cody\", \"trotter\", \"jingle\", \"augustine\", \"phineas\", \"estellas\", \"jaggerss\", \"wemmicks\", \"darcys\", \"lottys\", \"arbuthnots\", \"bartletts\", \"beebes\", \"leonards\", \"dixons\", \"lucas\", \"droitwiches\", \"bildad\", \"beckys\", \"redruth\", \"jem\", \"dinah\", \"westenra\", \"eshton\", \"sophie\", \"harry\", \"lorrys\", \"pumblechooks\", \"trabbs\", \"eleanors\", \"lydias\", \"bennets\", \"bourgh\", \"carraway\", \"sloane\", \"beppo\", \"lumley\", \"dowler\", \"peters\", \"peleg\", \"arley\", \"george\", \"defarges\", \"generals\", \"fitzwilliam\", \"hurst\", \"wickhams\", \"briggss\", \"gatz\", \"adolph\", \"poole\", \"morton\", \"abbot\", \"smollett\", \"gunn\", \"capn\", \"holbrook\", \"signor\", \"fitzadam\", \"mulliner\", \"brunoni\", \"barkers\", \"moffat\", \"dr\", \"andy\", \"barker\", \"jenkynss\", \"forresters\"\n)\n\n# Filter out character names and get thematic words\nthematic_tfidf &lt;- corpus_tfidf %&gt;%\n  filter(!word %in% character_names) %&gt;%\n  group_by(title) %&gt;%\n  slice_max(tf_idf, n = 8) %&gt;%\n  ungroup() %&gt;%\n  # Add genre information\n  left_join(corpus %&gt;% select(title, genre), by = \"title\")\n\n# Display thematic fingerprints by book\nthematic_tfidf %&gt;%\n  select(title, word, tf_idf, genre) %&gt;%\n  mutate(\n    title = str_trunc(title, 25),\n    tf_idf = round(tf_idf, 4)\n  ) %&gt;%\n  group_by(title, genre) %&gt;%\n  summarise(\n    thematic_words = paste(word, collapse = \", \"),\n    avg_tfidf = round(mean(tf_idf), 4),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(genre, title) %&gt;%\n  knitr::kable(\n    col.names = c(\"Book Title\", \"Genre\", \"Thematic Words\", \"Avg TF-IDF\"),\n    caption = \"Thematic fingerprints: distinctive non-character words by book\"\n  )\n\n\nThematic fingerprints: distinctive non-character words by book\n\n\n\n\n\n\n\n\nBook Title\nGenre\nThematic Words\nAvg TF-IDF\n\n\n\n\nAround the World in Ei…\nAdventure\nhong, kong, detective, yokohama, bombay, steamer, carnatic, mongolia\n0.0054\n\n\nMoby Dick\nAdventure\nwhale, sperm, pequod, nantucket, leviathan, harpoon, ye, whaling\n0.0065\n\n\nTom Sawyer\nAdventure\naint, aunt, muff, welshman, auntie, hanted, finn, becuz\n0.0023\n\n\nTreasure Island\nAdventure\nhispaniola, squire, stockade, coracle, benbow, joyce, rum, buccaneer\n0.0038\n\n\nCranford\nDomestic Fiction\npole, rector, maam, drumble, miss, jenny, betty, gordon\n0.0017\n\n\nLittle Women\nDomestic Fiction\n1869, tailpiece, 16mo, haf, aunt, grandpa, alcott, flo\n0.0016\n\n\nAlice’s Adventures in …\nFantasy\nhare, whiting, mock, dodo, lory, soooop, hedgehog, queen\n0.0030\n\n\nPeter Pan\nFantasy\ntootle, redskin, nib, pirate, lagoon, neverland, darling, tinker\n0.0057\n\n\nThe Wind in the Willows\nFantasy\notter, motorcar, bargewoman, gipsy, stoat, washerwoman, weasel, enginedriver, fieldmouse\n0.0025\n\n\nDracula\nGothic\nxury, friday, canoe, brazils, island, ship, boat, deliverance\n0.0024\n\n\nFrankenstein; Or, The …\nGothic\ngeneva, dæmon, ingolstadt, endeavour, kirwin, fiend, cottage, cottager\n0.0016\n\n\nJane Eyre\nGothic\nlydgate, dorothea, casaubon, bulstrode, rosamond, garth, middlemarch, celia\n0.0158\n\n\nGulliver’s Travels\nSatirical Fiction\nblefuscu, majesty, emperor, lilliput, cent, edit, manmountain, honor\n0.0032\n\n\nA Journey to the Centr…\nScience Fiction\nuncle, sneffels, professor, iceland, lava, granite, raft, crater\n0.0041\n\n\nThe Time Machine\nScience Fiction\nutterson, jekyll, hyde, lanyon, lawyer, jekylls, enfield, edward\n0.0172\n\n\nTwenty Thousand League…\nScience Fiction\nnautilus, meter, canadian, nautiluss, underwater, whale, professor, tank\n0.0076\n\n\nA Room with a View\nSocial Fiction\nflorence, lavish, italy, croce, tennis, arno, tunbridge, baedeker\n0.0027\n\n\nA Tale of Two Cities\nSocial Fiction\nmonseigneur, monsieur, wineshop, madame, evrémonde, barsad, mender, gabelle\n0.0036\n\n\nGreat Expectations\nSocial Fiction\nprovis, startop, camilla, marsh, skiffins, convict, trabb, walworth\n0.0018\n\n\nNorth and South\nSocial Fiction\nyer, ar, nigger, missis, thar, ye, dat, sambo\n0.0033\n\n\nNorthanger Abbey\nSocial Fiction\nyahoos, houyhnhnms, yahoo, majestys, blefuscu, majesty, emperor, houyhnhnm, luggnagg\n0.0038\n\n\nPride and Prejudice\nSocial Fiction\nlongbourn, netherfield, gardiner, rosings, hertfordshire, 1894, elizabeths, philips\n0.0042\n\n\nThe Enchanted April\nSocial Fiction\nfishers, hampstead, san, mezzago, mediaeval, suitcase, battlements, italy\n0.0027\n\n\nThe Great Gatsby\nSocial Fiction\ncar, garage, telephone, chicago, phone, coupé, wilsons, biloxi, drugstore\n0.0030\n\n\nThe Pickwick Papers\nSocial Fiction\nwery, serjeant, genlmn, pell, stiggins, nupkins, vith, wos\n0.0028\n\n\n\n\n\nLet’s visualize these thematic patterns:\n\n# Create thematic fingerprint visualization\nthematic_tfidf %&gt;%\n  mutate(\n    title_short = case_when(\n      str_detect(title, \"Pride and Prejudice\") ~ \"Pride & Prejudice\",\n      str_detect(title, \"Alice\") ~ \"Alice in Wonderland\",\n      str_detect(title, \"Great Gatsby\") ~ \"Great Gatsby\",\n      str_detect(title, \"Frankenstein\") ~ \"Frankenstein\",\n      str_detect(title, \"Journey.*Centre\") ~ \"Journey to Earth's Centre\",\n      str_detect(title, \"Twenty Thousand\") ~ \"20,000 Leagues\",\n      str_detect(title, \"Around.*World\") ~ \"Around World in 80 Days\",\n      TRUE ~ str_trunc(title, 20)\n    )\n  ) %&gt;%\n  ggplot(aes(x = reorder_within(word, tf_idf, title_short), y = tf_idf, fill = genre)) +\n  geom_col(alpha = 0.8) +\n  scale_x_reordered() +\n  coord_flip() +\n  facet_wrap(~title_short, scales = \"free\", ncol = 3) +\n  labs(\n    title = \"Thematic Fingerprints: Distinctive Non-Character Words\",\n    subtitle = \"What themes and concepts define each literary work?\",\n    x = \"Thematic Words\",\n    y = \"TF-IDF Score\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 9),\n    axis.text = element_text(size = 7)\n  ) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nGenre Thematic Fingerprints\nRather than focusing on individual book uniqueness, let’s explore how TF-IDF can reveal the distinctive linguistic signatures of entire literary genres. By analyzing vocabulary patterns across books within each genre, we can identify the “thematic DNA” that characterizes different literary traditions:\n\n# Create genre-level thematic fingerprints\ngenre_fingerprint &lt;- corpus_tfidf %&gt;%\n  # Remove character names from the analysis\n  filter(!word %in% character_names) %&gt;%\n  # Add genre information\n  left_join(corpus %&gt;% select(title, genre), by = \"title\") %&gt;%\n  # Calculate average TF-IDF by genre and word\n  group_by(genre, word) %&gt;%\n  summarise(\n    avg_tf_idf = mean(tf_idf),\n    books_with_word = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  # Get top distinctive words per genre\n  group_by(genre) %&gt;%\n  slice_max(avg_tf_idf, n = 10) %&gt;%\n  summarise(\n    distinctive_words = paste(word, collapse = \", \"),\n    avg_distinctiveness = round(mean(avg_tf_idf), 4),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(avg_distinctiveness))\n\n# Display the results\nknitr::kable(genre_fingerprint, \n             col.names = c(\"Genre\", \"Distinctive Vocabulary\", \"Avg TF-IDF\"),\n             caption = \"Genre thematic fingerprints: the linguistic DNA of literary traditions\")\n\n\nGenre thematic fingerprints: the linguistic DNA of literary traditions\n\n\n\n\n\n\n\nGenre\nDistinctive Vocabulary\nAvg TF-IDF\n\n\n\n\nScience Fiction\nutterson, jekyll, hyde, nautilus, meter, lanyon, canadian, uncle, nautiluss, lawyer\n0.0177\n\n\nGothic\nlydgate, dorothea, casaubon, bulstrode, rosamond, garth, middlemarch, celia, ladislaw, vincy\n0.0141\n\n\nAdventure\nwhale, hong, kong, hispaniola, sperm, yokohama, pequod, carnatic, mongolia, nantucket\n0.0076\n\n\nSocial Fiction\nyahoos, longbourn, monseigneur, houyhnhnms, netherfield, florence, yahoo, garage, fishers, gardiner\n0.0063\n\n\nFantasy\ntootle, redskin, nib, pirate, neverland, otter, darling, tinker, motorcar, whiting\n0.0051\n\n\nSatirical Fiction\nblefuscu, majesty, emperor, lilliput, cent, edit, manmountain, honor, imperial, empress\n0.0029\n\n\nDomestic Fiction\n1869, rector, tailpiece, drumble, 16mo, haf, aunt, gordon, carlo, marthas\n0.0017\n\n\n\n\n\n\n# Visualize genre distinctiveness\ngenre_word_analysis &lt;- corpus_tfidf %&gt;%\n  filter(!word %in% character_names) %&gt;%\n  left_join(corpus %&gt;% select(title, genre), by = \"title\") %&gt;%\n  group_by(genre, word) %&gt;%\n  summarise(avg_tf_idf = mean(tf_idf), .groups = \"drop\") %&gt;%\n  group_by(genre) %&gt;%\n  slice_max(avg_tf_idf, n = 8)\n\nggplot(genre_word_analysis, aes(x = reorder_within(word, avg_tf_idf, genre), \n                                y = avg_tf_idf, fill = genre)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  facet_wrap(~genre, scales = \"free\") +\n  coord_flip() +\n  labs(title = \"Most Distinctive Words by Literary Genre\",\n       subtitle = \"TF-IDF reveals the vocabulary that defines literary traditions\",\n       x = \"Words\", y = \"Average TF-IDF Score\") +\n  theme_minimal() +\n  theme(strip.text = element_text(size = 10, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenre Vocabulary Patterns\n\n\n\nThis analysis reveals fascinating patterns in how different literary genres employ distinctive vocabularies:\n\nAdventure Fiction: Geographic terms, maritime vocabulary, and travel-related words\nScience Fiction: Technological terminology, scientific concepts, and futuristic elements\nGothic Literature: Atmospheric terms, supernatural vocabulary, and dark settings\nSocial Fiction: Dialect markers, social terminology, and contemporary references\nFantasy: Magical terms, mythical vocabulary, and imaginative concepts\n\n\n\n\n\nWhat Genre Analysis Reveals\nThis genre-level TF-IDF analysis demonstrates several important insights:\n\nShared Literary Traditions: Authors within genres draw from common vocabularies that create recognizable literary voices\nThematic Coherence: Genre fingerprints reveal not just stylistic choices but fundamental thematic concerns\nReader Expectations: These vocabulary patterns help explain how readers can often identify genres from brief excerpts\nCultural Evolution: Genre vocabularies reflect the social, technological, and cultural contexts in which they were written",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "05-TF-IDF.html#interpreting-tf-idf-results",
    "href": "05-TF-IDF.html#interpreting-tf-idf-results",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "Interpreting TF-IDF Results",
    "text": "Interpreting TF-IDF Results\nOur analysis reveals fascinating patterns in how different books and genres use distinctive vocabulary. Let’s examine the “thematic fingerprints” that TF-IDF has uncovered.\n\nIndividual Book Fingerprints\nEach book in our corpus has developed its own linguistic signature through distinctive vocabulary:\nScience Fiction works show the highest TF-IDF scores, with technical and fantastical terminology: - Twenty Thousand Leagues Under the Sea: nautilus, meter, canadian, underwater (avg 0.0077) - The Time Machine: morlocks, laboratory, sphinx, eloi (avg 0.0043) - A Journey to the Centre of the Earth: uncle, sneffels, professor, iceland (avg 0.0042)\nAdventure novels feature location-specific and maritime vocabulary: - Moby Dick: whale, sperm, pequod, nantucket, whaling (avg 0.0067) - Around the World in Eighty Days: hong, kong, yokohama, bombay, steamer (avg 0.0054) - Treasure Island: hispaniola, squire, buccaneer, rum (avg 0.0038)\nGothic literature emphasizes atmospheric and supernatural elements: - Dracula: diary, whitby, undead, carfax, varna (avg 0.0024) - Jane Eyre: thornfield, lowood, brocklehurst, gateshead (avg 0.0021) - Frankenstein: geneva, dæmon, ingolstadt, creator (avg 0.0016)\n\n\nGenre-Level Patterns\nWhen we aggregate by genre, clear linguistic DNA emerges:\n\n\n\n\n\n\n\n\nGenre\nAvg TF-IDF\nCharacteristic Vocabulary\n\n\n\n\nScience Fiction\n0.0089\nnautilus, morlocks, meter, laboratory\n\n\nAdventure\n0.0078\nwhale, hispaniola, sperm, pequod\n\n\nSocial Fiction\n0.0053\nlongbourn, monseigneur, netherfield, garage\n\n\nFantasy\n0.0052\ntootle, redskin, pirate, neverland\n\n\nSatirical Fiction\n0.0036\nblefuscu, majesty, emperor, lilliput\n\n\nGothic\n0.0027\nthornfield, whitby, geneva, dracula\n\n\nDomestic Fiction\n0.0017\nrector, drumble, aunt, parlor\n\n\n\n\n\nWhat These Patterns Reveal\n\nGenre Hierarchies: Science Fiction and Adventure show the highest distinctiveness, suggesting these genres rely heavily on specialized vocabulary\nTemporal Markers: Words like telephone, car, garage in The Great Gatsby clearly mark 20th-century Social Fiction\nSetting Specificity: Geographic terms (florence, yokohama, nantucket) create strong textual fingerprints\nCharacter Integration: Names seamlessly blend with thematic vocabulary (e.g., darling in Peter Pan, dracula in Gothic fiction)\n\n\n\nPractical Applications\nThese thematic fingerprints enable:\n\nAuthorship Attribution: Distinctive vocabulary patterns help identify authors\nGenre Classification: Clear linguistic markers distinguish literary traditions\nContent Analysis: Specialized vocabularies reveal thematic preoccupations\nCultural Analysis: Word choices reflect historical and social contexts",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "05-TF-IDF.html#conclusion",
    "href": "05-TF-IDF.html#conclusion",
    "title": "TF-IDF: Finding Distinctive Words",
    "section": "Conclusion",
    "text": "Conclusion\nTF-IDF provides a powerful lens for understanding what makes texts distinctive within a corpus. Unlike simple frequency analysis, it reveals the words that truly characterize individual works, authors, genres, or time periods.\nOur analysis demonstrates how TF-IDF successfully creates “thematic fingerprints” for literature:\n\nScience Fiction emerges as the most linguistically distinctive genre (avg TF-IDF: 0.0089), with specialized technical vocabulary\nAdventure and Fantasy genres show strong thematic coherence through setting-specific and imaginative terminology\n\nSocial Fiction spans multiple time periods, from Austen’s longbourn to Fitzgerald’s garage\nGothic literature maintains consistent atmospheric vocabulary across different authors and periods\n\nThese linguistic signatures reveal how genres develop their own vocabularies and how individual works contribute to broader literary traditions. Our careful curation of the corpus demonstrates how analytical decisions shape results while maintaining scholarly integrity.\n\n\n\n\n\n\nMethodological Considerations\n\n\n\nCorpus Curation Matters: TF-IDF analysis is shaped by both technical parameters and curatorial choices. The resulting thematic fingerprints reflect not just linguistic patterns but also our analytical goals and values as researchers.\n\n\nThese insights form the foundation for more sophisticated analyses like document similarity measures and topic modeling, which we’ll explore in the upcoming chapters.\n\n\n\n\n\n\nPractice Exercises\n\n\n\nTry these exercises to deepen your understanding:\n\nExpand the Corpus: Add more books and see how TF-IDF results change\nExperiment with Preprocessing: Try including or excluding proper nouns, numbers, or different word types\nAuthor Comparison: Calculate TF-IDF for authors with multiple works in your corpus\nCustom Stop Words: Create domain-specific stop word lists to filter out less meaningful high-frequency terms\n\nRemember: TF-IDF is most informative when you have a diverse corpus with clear differences between documents.\n\n\nIn the next chapter, we’ll use these TF-IDF insights to explore text similarity measures, learning how to quantify how similar or different texts are from one another using techniques like cosine similarity and Euclidean distance.",
    "crumbs": [
      "5. TF-IDF Analysis"
    ]
  },
  {
    "objectID": "02-CreatingACorpus.html",
    "href": "02-CreatingACorpus.html",
    "title": "Creating a Corpus",
    "section": "",
    "text": "In this episode, we’re going to create a corpus of texts and prepare it for analysis by performing some standard preprocessing. Because of the nature of R, we will be transforming plain text files into tibbles, which are a modern take on data frames that make data manipulation easier and more intuitive.\nTo better understand what happens when we process a series of books, let’s start with an exercise using Kafka’s short story “Before the Law.”\nFirst, we need to install and load the tidyverse package, which includes dplyr, ggplot2, and other useful packages for data manipulation and visualization.\nIn RStudio, create a new R script (File → New File → R Script) and write the following commands. You can execute each line by placing your cursor on it and pressing Ctrl+Enter:\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nNext, we’ll create a text variable that stores our short story. We’re using tryCatch() to handle any potential errors when reading the file, and the read_lines() function, which is part of the readr package, to read the text file line by line safely.\nbeforethelaw_url &lt;- \"https://raw.githubusercontent.com/jairomelo/text-analysis-workshop/refs/heads/main/episodes/texts/Kafka-beforethelaw.txt\"\n\nbeforethelaw &lt;- tryCatch(\n    read_lines(beforethelaw_url, locale = locale(encoding = \"UTF-8\")),\n    error = function(e) {\n        message(\"Error reading the text: \", e)\n        return(NULL)\n    }\n)\n\nbeforethelaw\n\n[1] \"BEFORE THE LAW\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n[2] \"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n[3] \"Before the law sits a gatekeeper. To this gatekeeper comes a man from the country who asks to gain entry into the law. But the gatekeeper says that he cannot grant him entry at the moment. The man thinks about it and then asks if he will be allowed to come in sometime later on. “It is possible,” says the gatekeeper, “but not now.” The gate to the law stands open, as always, and the gatekeeper walks to the side, so the man bends over in order to see through the gate into the inside. When the gatekeeper notices that, he laughs and says: “If it tempts you so much, try going inside in spite of my prohibition. But take note. I am powerful. And I am only the lowliest gatekeeper. But from room to room stand gatekeepers, each more powerful than the last. I cannot endure even one glimpse of the third.” The man from the country has not expected such difficulties: the law should always be accessible for everyone, he thinks, but as he now looks more closely at the gatekeeper in his fur coat, at his large pointed nose and his long, thin, black Tartar’s beard, he decides that it would be better to wait until he gets permission to go inside. The gatekeeper gives him a stool and allows him to sit down at the side in front of the gate. There he sits for days and years. He makes many attempts to be let in, and he wears the gatekeeper out with his requests. The gatekeeper often interrogates him briefly, questioning him about his homeland and many other things, but they are indifferent questions, the kind great men put, and at the end he always tells him once more that he cannot let him inside yet. The man, who has equipped himself with many things for his journey, spends everything, no matter how valuable, to win over the gatekeeper. The latter takes it all but, as he does so, says, “I am taking this only so that you do not think you have failed to do anything.” During the many years the man observes the gatekeeper almost continuously. He forgets the other gatekeepers, and this first one seems to him the only obstacle for entry into the law. He curses the unlucky circumstance, in the first years thoughtlessly and out loud; later, as he grows old, he only mumbles to himself. He becomes childish and, since in the long years studying the gatekeeper he has also come to know the fleas in his fur collar, he even asks the fleas to help him persuade the gatekeeper. Finally his eyesight grows weak, and he does not know whether things are really darker around him or whether his eyes are merely deceiving him. But he recognizes now in the darkness an illumination which breaks inextinguishably out of the gateway to the law. Now he no longer has much time to live. Before his death he gathers up in his head all his experiences of the entire time into one question which he has not yet put to the gatekeeper. He waves to him, since he can no longer lift up his stiffening body. The gatekeeper has to bend way down to him, for the difference between them has changed considerably to the disadvantage of the man. “What do you want to know now?” asks the gatekeeper. “You are insatiable.” “Everyone strives after the law,” says the man, “so how is it that in these many years no one except me has requested entry?” The gatekeeper sees that the man is already dying and, in order to reach his diminishing sense of hearing, he shouts at him, “Here no one else can gain entry, since this entrance was assigned only to you. I’m going now to close it.”\"\nNow that we have our text stored as a character vector, we can transform it into a tibble. This process includes removing the title and filtering out empty lines.\nTo remove the title, we can use the str_remove_all() function from the stringr package. Our approach in this case is very direct, as we are matching the exact title line.\nTo remove the empty lines we can use the filter() function from dplyr to match non-empty strings.\nFinally, because we’re transforming the character vector into a tibble, it’s important to add an ID column to preserve the original line order and uniquely identify each line.\nbeforethelaw &lt;- beforethelaw %&gt;%\n    str_remove_all(\"^BEFORE THE LAW$\") # Remove the exact title line (^ = start, $ = end)\n\nbeforethelaw_tibble &lt;- tibble(text = beforethelaw) %&gt;%\n    filter(text != \"\") %&gt;% # Remove empty lines\n    mutate(id = row_number()) # Add an ID column to preserve line order\n\nbeforethelaw_tibble\n\n# A tibble: 1 × 2\n  text                                                                        id\n  &lt;chr&gt;                                                                    &lt;int&gt;\n1 Before the law sits a gatekeeper. To this gatekeeper comes a man from t…     1\nThe result is a two-column tibble: one for the text and another for the line ID.\nNow, we can proceed to perform some preprocessing steps to prepare our text for analysis. This includes converting the text to lowercase, removing punctuation, and tokenizing the text into individual words.",
    "crumbs": [
      "2. Creating a Corpus"
    ]
  },
  {
    "objectID": "02-CreatingACorpus.html#preprocessing-steps",
    "href": "02-CreatingACorpus.html#preprocessing-steps",
    "title": "Creating a Corpus",
    "section": "Preprocessing Steps",
    "text": "Preprocessing Steps\nAlthough a text can be analyzed in its raw form, preprocessing is essential for improving the quality of the analysis. Here are the key steps we’ll perform:\n\nConvert to Lowercase: This helps to standardize the text and avoid treating the same words with different cases as distinct.\nRemove Punctuation: Punctuation marks can interfere with text analysis (e.g., “gatekeeper.” would be considered different from “gatekeeper”), so we’ll remove them.\nTokenization: This is the process of splitting the text into individual words or tokens, which is crucial for most text analysis tasks.\n\nLet’s implement these preprocessing steps using dplyr, stringr, and tidytext. This approach allows us to chain multiple operations together:\n\nlibrary(tidytext) # We'll need this for tokenization\n\nbeforethelaw_preprocessed &lt;- beforethelaw_tibble %&gt;%\n    mutate(text = str_to_lower(text)) %&gt;%        # Convert to lowercase\n    mutate(text = str_remove_all(text, \"[[:punct:]]\")) %&gt;%  # Remove punctuation\n    unnest_tokens(word, text)                    # Tokenization into individual words\n\nbeforethelaw_preprocessed\n\n# A tibble: 640 × 2\n      id word      \n   &lt;int&gt; &lt;chr&gt;     \n 1     1 before    \n 2     1 the       \n 3     1 law       \n 4     1 sits      \n 5     1 a         \n 6     1 gatekeeper\n 7     1 to        \n 8     1 this      \n 9     1 gatekeeper\n10     1 comes     \n# ℹ 630 more rows\n\n\nWe’ve transformed our single-row tibble into a long 640-row tibble, where each row represents a single word from the original text. The unnest_tokens() function from the tidytext package is particularly useful here, as it automatically handles tokenization and creates a new row for each word.\nNote that this number of rows matches the word count from the previous episode. However, this alignment won’t always occur. Depending on text complexity, basic word counting and tokenization can produce different results. For example, tokenization might split contractions (“don’t” → “don” + “t”) or treat hyphenated words (“twenty-one”) as separate tokens, while a simple word count would treat them as single units.\nDepending on the purposes of your analysis, you may want to perform additional preprocessing steps, such as removing stop words (common words like “the,” “and,” “is” that may not carry significant meaning for analysis). Let’s remove stop words for this example using the tidytext package:\n\nbeforethelaw_reduced &lt;- beforethelaw_preprocessed %&gt;%\n    anti_join(stop_words, by = \"word\") # anti_join removes words that match the stop_words list\n\nbeforethelaw_reduced\n\n# A tibble: 164 × 2\n      id word      \n   &lt;int&gt; &lt;chr&gt;     \n 1     1 law       \n 2     1 sits      \n 3     1 gatekeeper\n 4     1 gatekeeper\n 5     1 country   \n 6     1 gain      \n 7     1 entry     \n 8     1 law       \n 9     1 gatekeeper\n10     1 grant     \n# ℹ 154 more rows\n\n\nThis new tibble has 164 rows, meaning we’ve reduced our original text by 75% by removing stop words. This is important to keep in mind, as preprocessing can improve analysis quality by reducing noise and focusing on relevant terms, but it can also result in a loss of context or meaning that may be important for certain types of analysis.\n\n\n\n\n\n\nNote\n\n\n\nStop word removal isn’t always necessary. For tasks like authorship attribution or sentiment analysis, these “common” words can actually provide valuable information about writing style or tone.",
    "crumbs": [
      "2. Creating a Corpus"
    ]
  },
  {
    "objectID": "02-CreatingACorpus.html#create-a-corpus",
    "href": "02-CreatingACorpus.html#create-a-corpus",
    "title": "Creating a Corpus",
    "section": "Create a Corpus",
    "text": "Create a Corpus\nThe term “corpus” is borrowed from Latin. The main meaning of that word is “body,” but in medieval Latin, a collection of writings about the same topic were also called “corpus” (e.g. the “Corpus Juris Civilis”). In text analysis, we use this term more broadly to refer to the entire body of text that we are analyzing, but usually it implies a previous curation of what texts are included so that they share a common theme, time period, or author. Corpora can range from a few documents to millions of texts, depending on the research question.\nWe’ve curated a corpus of classic works in the public domain. These works share some common themes, styles, and historical contexts, making them suitable for comparative analysis.\nWe have included 26 works by 20 authors, from a period spanning from 1726 to 1925, although, most works are from the 19th century. We aim to create a corpus useful for demonstration purposes while keeping some consistency in terms of length and genre to allow for meaningful comparisons.\nThe glID column contains Project Gutenberg identification numbers, which can be used to construct URLs for downloading the full texts.\n\ncorpus &lt;- tribble(\n  ~author, ~title, ~year, ~glID, ~genre,\n  \"Jane Austen\", \"Pride and Prejudice\", 1813, 1342, \"Social Fiction\",\n  \"Charles Dickens\", \"A Tale of Two Cities\", 1859, 98, \"Social Fiction\",\n  \"F. Scott Fitzgerald\", \"The Great Gatsby\", 1925, 64317, \"Social Fiction\",\n  \"Mary Wollstonecraft Shelley\", \"Frankenstein; Or, The Modern Prometheus\", 1818, 84, \"Gothic\",\n  \"Herman Melville\", \"Moby Dick\", 1851, 2701, \"Adventure\",\n  \"Louisa May Alcott\", \"Little Women\", 1868, 37106, \"Domestic Fiction\",\n  \"Mark Twain\", \"Tom Sawyer\", 1876, 74, \"Adventure\",\n  \"Jonathan Swift\", \"Gulliver's Travels\", 1726, 17157, \"Satirical Fiction\",\n  \"E. M. Forster\", \"A Room with a View\", 1908, 2641, \"Social Fiction\",\n  \"Elizabeth Von Arnim\", \"The Enchanted April\", 1922, 16389, \"Social Fiction\",\n  \"Lewis Carroll\", \"Alice's Adventures in Wonderland\", 1865, 11, \"Fantasy\",\n  \"Elizabeth Gaskell\", \"Cranford\", 1853, 394, \"Domestic Fiction\",\n  \"Charles Dickens\", \"The Pickwick Papers\", 1836, 580, \"Social Fiction\",\n  \"J. M. Barrie\", \"Peter Pan\", 1911, 16, \"Fantasy\",\n  \"Charles Dickens\", \"Great Expectations\", 1861, 1400, \"Social Fiction\",\n  \"Robert Louis Stevenson\", \"Treasure Island\", 1883, 120, \"Adventure\",\n  \"Kenneth Grahame\", \"The Wind in the Willows\", 1908, 27805, \"Fantasy\",\n  \"Jules Verne\", \"Twenty Thousand Leagues Under the Seas\", 1870, 2488, \"Science Fiction\",\n  \"Jules Verne\", \"A Journey to the Centre of the Earth\", 1864, 18857, \"Science Fiction\",\n  \"Jules Verne\", \"Around the World in Eighty Days\", 1873, 103, \"Adventure\",\n  \"Bram Stoker\", \"Dracula\", 1897, 345, \"Gothic\",\n  \"H. G. Wells\", \"The Time Machine\", 1895, 35, \"Science Fiction\",\n  \"Charlotte Brontë\", \"Jane Eyre\", 1847, 1260, \"Gothic\",\n  \"Jane Austen\", \"Northanger Abbey\", 1817, 121, \"Social Fiction\",\n  \"Elizabeth Gaskell\", \"North and South\", 1855, 4276, \"Social Fiction\"\n)\n\ncorpus\n\n# A tibble: 25 × 5\n   author                      title                            year  glID genre\n   &lt;chr&gt;                       &lt;chr&gt;                           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n 1 Jane Austen                 Pride and Prejudice              1813  1342 Soci…\n 2 Charles Dickens             A Tale of Two Cities             1859    98 Soci…\n 3 F. Scott Fitzgerald         The Great Gatsby                 1925 64317 Soci…\n 4 Mary Wollstonecraft Shelley Frankenstein; Or, The Modern P…  1818    84 Goth…\n 5 Herman Melville             Moby Dick                        1851  2701 Adve…\n 6 Louisa May Alcott           Little Women                     1868 37106 Dome…\n 7 Mark Twain                  Tom Sawyer                       1876    74 Adve…\n 8 Jonathan Swift              Gulliver's Travels               1726 17157 Sati…\n 9 E. M. Forster               A Room with a View               1908  2641 Soci…\n10 Elizabeth Von Arnim         The Enchanted April              1922 16389 Soci…\n# ℹ 15 more rows\n\n\nWe can have a glimpse of the corpus by examining how many authors and how many genres are represented.\n\n# Examine the corpus\ncorpus %&gt;%\n  count(author, sort = TRUE)\n\n# A tibble: 19 × 2\n   author                          n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Charles Dickens                 3\n 2 Jules Verne                     3\n 3 Elizabeth Gaskell               2\n 4 Jane Austen                     2\n 5 Bram Stoker                     1\n 6 Charlotte Brontë                1\n 7 E. M. Forster                   1\n 8 Elizabeth Von Arnim             1\n 9 F. Scott Fitzgerald             1\n10 H. G. Wells                     1\n11 Herman Melville                 1\n12 J. M. Barrie                    1\n13 Jonathan Swift                  1\n14 Kenneth Grahame                 1\n15 Lewis Carroll                   1\n16 Louisa May Alcott               1\n17 Mark Twain                      1\n18 Mary Wollstonecraft Shelley     1\n19 Robert Louis Stevenson          1\n\ncorpus %&gt;%\n  count(genre, sort = TRUE)\n\n# A tibble: 7 × 2\n  genre                 n\n  &lt;chr&gt;             &lt;int&gt;\n1 Social Fiction        9\n2 Adventure             4\n3 Fantasy               3\n4 Gothic                3\n5 Science Fiction       3\n6 Domestic Fiction      2\n7 Satirical Fiction     1\n\n\nWe can observe how our corpus contains works by mostly individual authors, although with some exceptions as Dickens, Verne, Gaskell and Austen. In terms of genres, we have a diverse range, with a strong representation of Social Fiction, Adventure, and Fantasy, alongside notable works in Gothic and Science Fiction.\nAnother relevant point about this corpus is that it represents what became the “standard” English-language literary canon, even with the inclusion of Verne’s works, which were originally written in French but became deeply embedded in English-language reading culture through translation. This corpus is a good starting point for exploring the evolution of English literature and the themes that have persisted over time.\n\nFetch the Texts\nOur next step is to fetch the full text that constitutes our corpus. We will use the glID (Gutenberg Library ID) to retrieve the texts from the Project Gutenberg website. To facilitate this process, we can use the gutenbergr package in R, which handles the downloading and cleaning automatically.\nStep by step, what we are going to do is install and load the gutenbergr package, which handles the downloading and cleaning automatically (removing Project Gutenberg headers, footers, and formatting markers).\n\ninstall.packages(\"gutenbergr\") # Install if needed (run once)\nlibrary(gutenbergr)\n\nAfter that, we need to iterate over the glIDs in our corpus for the package to know what book to download. The function gutenberg_download() will take the glID as an argument and return the full text of the book as a tibble with two columns, like this one:\n\ngutenberg_download(1342) # 1342 is the ID for \"Pride and Prejudice\"\n\nDetermining mirror for Project Gutenberg from\nhttps://www.gutenberg.org/robot/harvest.\nUsing mirror http://aleph.gutenberg.org.\n\n\n# A tibble: 14,527 × 2\n   gutenberg_id text                                            \n          &lt;int&gt; &lt;chr&gt;                                           \n 1         1342 \"                             GEORGE ALLEN\"     \n 2         1342 \"                               PUBLISHER\"      \n 3         1342 \"\"                                              \n 4         1342 \"                        156 CHARING CROSS ROAD\"\n 5         1342 \"                                LONDON\"        \n 6         1342 \"\"                                              \n 7         1342 \"                             RUSKIN HOUSE\"     \n 8         1342 \"                                   ]\"          \n 9         1342 \"\"                                              \n10         1342 \"                            [Illustration:\"    \n# ℹ 14,517 more rows\n\n\nThis tibble needs to be transformed into a single text string that fits in a column of our corpus tibble. We can use the pull() function to extract the text column and then use paste(collapse = \" \") to concatenate all the lines into a single string.\n\n# Download all books at once\ncorpus_texts &lt;- corpus %&gt;%\n   mutate(text = map(glID, ~gutenberg_download(.x) %&gt;%\n   pull(text) %&gt;%\n   paste(collapse = \" \")))\n\ncorpus_texts\n\n# A tibble: 25 × 6\n   author                      title                      year  glID genre text \n   &lt;chr&gt;                       &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lis&gt;\n 1 Jane Austen                 Pride and Prejudice        1813  1342 Soci… &lt;chr&gt;\n 2 Charles Dickens             A Tale of Two Cities       1859    98 Soci… &lt;chr&gt;\n 3 F. Scott Fitzgerald         The Great Gatsby           1925 64317 Soci… &lt;chr&gt;\n 4 Mary Wollstonecraft Shelley Frankenstein; Or, The Mo…  1818    84 Goth… &lt;chr&gt;\n 5 Herman Melville             Moby Dick                  1851  2701 Adve… &lt;chr&gt;\n 6 Louisa May Alcott           Little Women               1868 37106 Dome… &lt;chr&gt;\n 7 Mark Twain                  Tom Sawyer                 1876    74 Adve… &lt;chr&gt;\n 8 Jonathan Swift              Gulliver's Travels         1726 17157 Sati… &lt;chr&gt;\n 9 E. M. Forster               A Room with a View         1908  2641 Soci… &lt;chr&gt;\n10 Elizabeth Von Arnim         The Enchanted April        1922 16389 Soci… &lt;chr&gt;\n# ℹ 15 more rows\n\n\n\n\n\n\n\n\nUnderstanding the map() function\n\n\n\n\n\nThe map() function from the purrr package (part of tidyverse) applies a function to each element of a vector or list. In our case:\n\nmap(glID, ~...) takes each ID from the glID column\nThe ~ creates an anonymous function (similar to a lambda function in Python)\n.x represents each individual ID as map() processes them one by one\nSo ~gutenberg_download(.x) means “for each ID, download that book”\n\nThis is equivalent to writing a for loop, but more concise and functional programming style. The result is that each book gets downloaded and processed, creating a new text column in our tibble.\nIf you’re new to map(), think of it as “do this same operation to every item in this list.”\n\n\n\nIn the end, we will have a corpus_texts tibble that contains the full text of each book in our corpus as a character vector stored in the text column.\n\n\nCleaning\nThe texts provided by the Gutenberg Library are almost ready for analysis. However, we still need to perform some basic cleaning steps to ensure consistency and remove any unwanted artifacts.\n\nUnderstanding Preprocessing Decisions\nBefore applying cleaning steps mechanically, it’s important to understand how our preprocessing choices affect the results. A common challenge is handling possessive forms properly.\nWhen we remove all punctuation indiscriminately, we transform possessive forms in ways that might not be intended:\n\n# Example text to demonstrate the issue\nexample_text &lt;- \"Beth's book and Joe's pen were left at Mary's house.\"\n\n# Method 1: Remove all punctuation (problematic)\nsimple_clean &lt;- example_text %&gt;%\n  str_to_lower() %&gt;%\n  str_remove_all(\"[[:punct:]]\")\n\ncat(\"Original:\", example_text, \"\\n\")\n\nOriginal: Beth's book and Joe's pen were left at Mary's house. \n\ncat(\"Simple cleaning:\", simple_clean, \"\\n\")\n\nSimple cleaning: beths book and joes pen were left at marys house \n\n\nThe simple approach transforms “Beth’s” into “beths”, “Joe’s” into “joes”, etc. We can handle this more elegantly during tokenization by removing the possessive suffix after we’ve split the text into words.\n\n\n\n\n\n\nPreprocessing Philosophy\n\n\n\nEvery preprocessing decision shapes your results. Always consider: - What linguistic information do you want to preserve? - How will your choices affect downstream analysis? - Can you make your preprocessing decisions explicit and reversible?\nFor possessives, we have several options: - Remove ‘s suffix: “Mary’s” → “Mary” (treats possessive as just a reference to the person/thing) - Expand to ’has’: “Mary’s book” → “Mary has book” (preserves relationship)\n- Keep as-is: Preserve the grammatical information\nFor most text analysis tasks, removing the suffix is the most practical approach.\n\n\n\n\nApplying Smart Cleaning\nLet’s apply a straightforward approach: remove punctuation normally, then handle possessives during tokenization:\n\n# Cleaning the texts - simple approach\ncorpus_texts &lt;- corpus_texts %&gt;%\n   mutate(\n    text_clean = str_to_lower(text) %&gt;%               # Convert to lowercase\n       str_remove_all(\"'s\\\\b\") %&gt;%                    # Remove possessive 's from full text\n       str_remove_all(\"[[:punct:]]\") %&gt;%              # Remove remaining punctuation\n       str_squish()                                    # Remove extra whitespace\n   )\n\ncorpus_texts\n\n# A tibble: 25 × 7\n   author                      title           year  glID genre text  text_clean\n   &lt;chr&gt;                       &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lis&gt; &lt;chr&gt;     \n 1 Jane Austen                 Pride and Pre…  1813  1342 Soci… &lt;chr&gt; george al…\n 2 Charles Dickens             A Tale of Two…  1859    98 Soci… &lt;chr&gt; a tale of…\n 3 F. Scott Fitzgerald         The Great Gat…  1925 64317 Soci… &lt;chr&gt; the great…\n 4 Mary Wollstonecraft Shelley Frankenstein;…  1818    84 Goth… &lt;chr&gt; frankenst…\n 5 Herman Melville             Moby Dick       1851  2701 Adve… &lt;chr&gt; mobydick …\n 6 Louisa May Alcott           Little Women    1868 37106 Dome… &lt;chr&gt; illustrat…\n 7 Mark Twain                  Tom Sawyer      1876    74 Adve… &lt;chr&gt; the adven…\n 8 Jonathan Swift              Gulliver's Tr…  1726 17157 Sati… &lt;chr&gt; gulliver …\n 9 E. M. Forster               A Room with a…  1908  2641 Soci… &lt;chr&gt; illustrat…\n10 Elizabeth Von Arnim         The Enchanted…  1922 16389 Soci… &lt;chr&gt; illustrat…\n# ℹ 15 more rows\n\n\nThis code creates a new column text_clean in our corpus_texts tibble, which contains the cleaned text ready for analysis. The str_squish() function is particularly useful as it removes any leading, trailing, or extra spaces between words, ensuring that our text is tidy and consistent.\nThese are basic cleaning steps that improve the quality of our text data and reduce noise that might interfere with our analysis. However, even after these steps, and even using a high-quality corpus like Project Gutenberg, we still face some challenges such as table of contents, chapter headers, editorial notes, and translator introductions.\nIn real-world projects, text cleaning often involves both automated processes and manual review to ensure corpus consistency. For our workshop, we’ll assume these texts are clean enough for analysis, but in practice, you may need to spend considerably more time refining your corpus.\nNow that we have a clean, standardized corpus of 26 classic works, we’re ready to begin our text analysis journey. In the next section, we’ll start with fundamental techniques: examining word frequencies, calculating lexical density, and measuring readability scores. These basic analyses will give us our first insights into the linguistic patterns that distinguish different authors, genres, and time periods in our corpus.",
    "crumbs": [
      "2. Creating a Corpus"
    ]
  }
]