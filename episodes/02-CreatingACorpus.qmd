---
title: "Creating a Corpus"
engine: knitr
format:
  html:
    fig-width: 10
    fig-height: 12
    dpi: 300
editor_options: 
  chunk_output_type: inline
---

In this episode, we're going to create a corpus of texts and prepare it for analysis by performing some standard preprocessing. Because of the nature of R, we will be transforming plain text files into tibbles, which are a modern take on data frames that make data manipulation easier and more intuitive.

::: {.callout-note title="Why not bags of words?" collapse="true"}
If you're coming from Python, your instinct might be to create lists (like numpy arrays) for text analysis. But R's strength lies in structured data frames. By using tibbles instead of simple word lists, we can preserve text relationships and leverage the full tidyverse ecosystem.
:::

To better understand what happens when we process a series of books, let's start with an exercise using Kafka's short story "Before the Law."

First, we need to install and load the `tidyverse` package, which includes `dplyr`, `ggplot2`, and other useful packages for data manipulation and visualization.

In RStudio, create a new R script (File → New File → R Script) and write the following commands. You can execute each line by placing your cursor on it and pressing Ctrl+Enter:

```{r}
#| output: false
install.packages("tidyverse")
library(tidyverse)
```

Next, we'll create a text variable that stores our short story. We're using `tryCatch()` to handle any potential errors when reading the file, and the `read_lines()` function, which is part of the `readr` package, to read the text file line by line safely.

```{r}
beforethelaw_url <- "https://raw.githubusercontent.com/jairomelo/text-analysis-workshop/refs/heads/main/episodes/texts/Kafka-beforethelaw.txt"

beforethelaw <- tryCatch(
    read_lines(beforethelaw_url, locale = locale(encoding = "UTF-8")),
    error = function(e) {
        message("Error reading the text: ", e)
        return(NULL)
    }
)

beforethelaw
```

Now that we have our text stored as a character vector, we can transform it into a tibble. This process includes removing the title and filtering out empty lines.

To remove the title, we can use the `str_remove_all()` function from the `stringr` package. Our approach in this case is very direct, as we are matching the exact title line.

To remove the empty lines we can use the `filter()` function from `dplyr` to match non-empty strings. 

Finally, because we're transforming the character vector into a tibble, it's important to add an ID column to preserve the original line order and uniquely identify each line.

```{r}

beforethelaw <- beforethelaw %>%
    str_remove_all("^BEFORE THE LAW$") # Remove the exact title line (^ = start, $ = end)

beforethelaw_tibble <- tibble(text = beforethelaw) %>%
    filter(text != "") %>% # Remove empty lines
    mutate(id = row_number()) # Add an ID column to preserve line order

beforethelaw_tibble
```

The result is a two-column tibble: one for the text and another for the line ID.

Now, we can proceed to perform some preprocessing steps to prepare our text for analysis. This includes converting the text to lowercase, removing punctuation, and tokenizing the text into individual words.

## Preprocessing Steps

Although a text can be analyzed in its raw form, preprocessing is essential for improving the quality of the analysis. Here are the key steps we'll perform:

1. **Convert to Lowercase**: This helps to standardize the text and avoid treating the same words with different cases as distinct.
2. **Remove Punctuation**: Punctuation marks can interfere with text analysis (e.g., "gatekeeper." would be considered different from "gatekeeper"), so we'll remove them.
3. **Tokenization**: This is the process of splitting the text into individual words or tokens, which is crucial for most text analysis tasks.

Let's implement these preprocessing steps using `dplyr`, `stringr`, and `tidytext`. This approach allows us to chain multiple operations together:

```{r}
library(tidytext) # We'll need this for tokenization

beforethelaw_preprocessed <- beforethelaw_tibble %>%
    mutate(text = str_to_lower(text)) %>%        # Convert to lowercase
    mutate(text = str_remove_all(text, "[[:punct:]]")) %>%  # Remove punctuation
    unnest_tokens(word, text)                    # Tokenization into individual words

beforethelaw_preprocessed
```

We've transformed our single-row tibble into a long 640-row tibble, where each row represents a single word from the original text. The `unnest_tokens()` function from the `tidytext` package is particularly useful here, as it automatically handles tokenization and creates a new row for each word. 

Note that this number of rows matches the word count from the previous episode. However, this alignment won't always occur. Depending on text complexity, basic word counting and tokenization can produce different results. For example, tokenization might split contractions ("don't" → "don" + "t") or treat hyphenated words ("twenty-one") as separate tokens, while a simple word count would treat them as single units.

Depending on the purposes of your analysis, you may want to perform additional preprocessing steps, such as removing stop words (common words like "the," "and," "is" that may not carry significant meaning for analysis). Let's remove stop words for this example using the `tidytext` package:

```{r}
beforethelaw_reduced <- beforethelaw_preprocessed %>%
    anti_join(stop_words, by = "word") # anti_join removes words that match the stop_words list

beforethelaw_reduced
```

::: {.callout-note}
 Stop word removal isn't always necessary. For tasks like authorship attribution or sentiment analysis, these "common" words can actually provide valuable information about writing style or tone.
:::

This new tibble has 164 rows, meaning we've reduced our original text by 75% by removing stop words. This is important to keep in mind, as preprocessing can improve analysis quality by reducing noise and focusing on relevant terms, but it can also result in a loss of context or meaning that may be important for certain types of analysis.