---
title: "Creating a Corpus"
engine: knitr
format:
  html:
    fig-width: 10
    fig-height: 12
    dpi: 300
editor_options: 
  chunk_output_type: inline
---

In this episode, we're going to create a corpus of texts and prepare it for analysis by performing some standard preprocessing. Because of the nature of R, we will be transforming plain text files into tibbles, which are a modern take on data frames that make data manipulation easier and more intuitive.

::: {.callout-note title="Why not bags of words?" collapse="true"}
If you're coming from Python, your instinct might be to create lists (like numpy arrays) for text analysis. But R's strength lies in structured data frames. By using tibbles instead of simple word lists, we can preserve text relationships and leverage the full tidyverse ecosystem.
:::

To better understand what happens when we process a series of books, let's start with an exercise using Kafka's short story "Before the Law."

First, we need to install and load the `tidyverse` package, which includes `dplyr`, `ggplot2`, and other useful packages for data manipulation and visualization.

In RStudio, create a new R script (File → New File → R Script) and write the following commands. You can execute each line by placing your cursor on it and pressing Ctrl+Enter:

```{r}
#| output: false
install.packages("tidyverse")
library(tidyverse)
```

Next, we'll create a text variable that stores our short story. We're using `tryCatch()` to handle any potential errors when reading the file, and the `read_lines()` function, which is part of the `readr` package, to read the text file line by line safely.

```{r}
beforethelaw_url <- "https://raw.githubusercontent.com/jairomelo/text-analysis-workshop/refs/heads/main/episodes/texts/Kafka-beforethelaw.txt"

beforethelaw <- tryCatch(
    read_lines(beforethelaw_url, locale = locale(encoding = "UTF-8")),
    error = function(e) {
        message("Error reading the text: ", e)
        return(NULL)
    }
)

beforethelaw
```

Now that we have our text stored as a character vector, we can transform it into a tibble. This process includes removing the title and filtering out empty lines.

To remove the title, we can use the `str_remove_all()` function from the `stringr` package. Our approach in this case is very direct, as we are matching the exact title line.

To remove the empty lines we can use the `filter()` function from `dplyr` to match non-empty strings. 

Finally, because we're transforming the character vector into a tibble, it's important to add an ID column to preserve the original line order and uniquely identify each line.

```{r}

beforethelaw <- beforethelaw %>%
    str_remove_all("^BEFORE THE LAW$") # Remove the exact title line (^ = start, $ = end)

beforethelaw_tibble <- tibble(text = beforethelaw) %>%
    filter(text != "") %>% # Remove empty lines
    mutate(id = row_number()) # Add an ID column to preserve line order

beforethelaw_tibble
```

The result is a two-column tibble: one for the text and another for the line ID.

Now, we can proceed to perform some preprocessing steps to prepare our text for analysis. This includes converting the text to lowercase, removing punctuation, and tokenizing the text into individual words.

## Preprocessing Steps

Although a text can be analyzed in its raw form, preprocessing is essential for improving the quality of the analysis. Here are the key steps we'll perform:

1. **Convert to Lowercase**: This helps to standardize the text and avoid treating the same words with different cases as distinct.
2. **Remove Punctuation**: Punctuation marks can interfere with text analysis (e.g., "gatekeeper." would be considered different from "gatekeeper"), so we'll remove them.
3. **Tokenization**: This is the process of splitting the text into individual words or tokens, which is crucial for most text analysis tasks.

Let's implement these preprocessing steps using `dplyr`, `stringr`, and `tidytext`. This approach allows us to chain multiple operations together:

```{r}
library(tidytext) # We'll need this for tokenization

beforethelaw_preprocessed <- beforethelaw_tibble %>%
    mutate(text = str_to_lower(text)) %>%        # Convert to lowercase
    mutate(text = str_remove_all(text, "[[:punct:]]")) %>%  # Remove punctuation
    unnest_tokens(word, text)                    # Tokenization into individual words

beforethelaw_preprocessed
```

We've transformed our single-row tibble into a long 640-row tibble, where each row represents a single word from the original text. The `unnest_tokens()` function from the `tidytext` package is particularly useful here, as it automatically handles tokenization and creates a new row for each word. 

Note that this number of rows matches the word count from the previous episode. However, this alignment won't always occur. Depending on text complexity, basic word counting and tokenization can produce different results. For example, tokenization might split contractions ("don't" → "don" + "t") or treat hyphenated words ("twenty-one") as separate tokens, while a simple word count would treat them as single units.

Depending on the purposes of your analysis, you may want to perform additional preprocessing steps, such as removing stop words (common words like "the," "and," "is" that may not carry significant meaning for analysis). Let's remove stop words for this example using the `tidytext` package:

```{r}
beforethelaw_reduced <- beforethelaw_preprocessed %>%
    anti_join(stop_words, by = "word") # anti_join removes words that match the stop_words list

beforethelaw_reduced
```

This new tibble has 164 rows, meaning we've reduced our original text by 75% by removing stop words. This is important to keep in mind, as preprocessing can improve analysis quality by reducing noise and focusing on relevant terms, but it can also result in a loss of context or meaning that may be important for certain types of analysis.

::: {.callout-note}
 Stop word removal isn't always necessary. For tasks like authorship attribution or sentiment analysis, these "common" words can actually provide valuable information about writing style or tone.
:::

## Create a Corpus

The term "corpus" is borrowed from Latin. The main meaning of that word is "body," but in medieval Latin, a collection of writings about the same topic were also called "corpus" (e.g. the "Corpus Juris Civilis"). In text analysis, we use this term more broadly to refer to the entire body of text that we are analyzing, but usually it implies a previous curation of what texts are included so that they share a common theme, time period, or author. Corpora can range from a few documents to millions of texts, depending on the research question.

We've curated a corpus of classic works in the public domain. These works share some common themes, styles, and historical contexts, making them suitable for comparative analysis.

We have included 26 works by 20 authors, from a period spanning from 1726 to 1925, although, most works are from the 19th century. We aim to create a corpus useful for demonstration purposes while keeping some consistency in terms of length and genre to allow for meaningful comparisons.

The `glID` column contains Project Gutenberg identification numbers, which can be used to construct URLs for downloading the full texts.

```{r}

corpus <- tribble(
  ~author, ~title, ~year, ~glID, ~genre,
  "Jane Austen", "Pride and Prejudice", 1813, 1342, "Social Fiction",
  "Charles Dickens", "A Tale of Two Cities", 1859, 98, "Social Fiction",
  "F. Scott Fitzgerald", "The Great Gatsby", 1925, 64317, "Social Fiction",
  "Mary Wollstonecraft Shelley", "Frankenstein; Or, The Modern Prometheus", 1818, 84, "Gothic",
  "Herman Melville", "Moby Dick", 1851, 2701, "Adventure",
  "Louisa May Alcott", "Little Women", 1868, 37106, "Domestic Fiction",
  "Mark Twain", "Tom Sawyer", 1876, 74, "Adventure",
  "Jonathan Swift", "Gulliver's Travels", 1726, 17157, "Satirical Fiction",
  "E. M. Forster", "A Room with a View", 1908, 2641, "Social Fiction",
  "Elizabeth Von Arnim", "The Enchanted April", 1922, 16389, "Social Fiction",
  "Lewis Carroll", "Alice's Adventures in Wonderland", 1865, 11, "Fantasy",
  "Elizabeth Gaskell", "Cranford", 1853, 394, "Domestic Fiction",
  "Charles Dickens", "The Pickwick Papers", 1836, 580, "Social Fiction",
  "J. M. Barrie", "Peter Pan", 1911, 16, "Fantasy",
  "Charles Dickens", "Great Expectations", 1861, 1400, "Social Fiction",
  "Harriet Beecher Stowe", "Uncle Tom's Cabin", 1852, 203, "Social Fiction",
  "Robert Louis Stevenson", "Treasure Island", 1883, 120, "Adventure",
  "Kenneth Grahame", "The Wind in the Willows", 1908, 27805, "Fantasy",
  "Jules Verne", "Twenty Thousand Leagues Under the Seas", 1870, 2488, "Science Fiction",
  "Jules Verne", "A Journey to the Centre of the Earth", 1864, 18857, "Science Fiction",
  "Jules Verne", "Around the World in Eighty Days", 1873, 103, "Adventure",
  "Bram Stoker", "Dracula", 1897, 345, "Gothic",
  "H. G. Wells", "The Time Machine", 1895, 35, "Science Fiction",
  "Charlotte Brontë", "Jane Eyre", 1847, 1260, "Gothic",
  "Jane Austen", "Northanger Abbey", 1817, 121, "Social Fiction",
  "Elizabeth Gaskell", "North and South", 1855, 4276, "Social Fiction"
)

corpus
```

We can have a glimpse of the corpus by examining how many authors and how many genres are represented.

```{r}
# Examine the corpus
corpus %>%
  count(author, sort = TRUE)

corpus %>%
  count(genre, sort = TRUE)
```

We can observe how our corpus contains works by mostly individual authors, although with some exceptions as Dickens, Verne, Gaskell and Austen. In terms of genres, we have a diverse range, with a strong representation of Social Fiction, Adventure, and Fantasy, alongside notable works in Gothic and Science Fiction.

Another relevant point about this corpus is that it represents what became the "standard" English-language literary canon, even with the inclusion of Verne's works, which were originally written in French but became deeply embedded in English-language reading culture through translation. This corpus is a good starting point for exploring the evolution of English literature and the themes that have persisted over time.

### Fetch the Texts

Our next step is to fetch the full text that constitutes our corpus. We will use the `glID` (Gutenberg Library ID) to retrieve the texts from the Project Gutenberg website. To facilitate this process, we can use the `gutenbergr` package in R, which handles the downloading and cleaning automatically.

Step by step, what we are going to do is install and load the gutenbergr package, which handles the downloading and cleaning automatically (removing Project Gutenberg headers, footers, and formatting markers). 

```{r}
#| output: false
install.packages("gutenbergr") # Install if needed (run once)
library(gutenbergr)
```

After that, we need to iterate over the `glIDs` in our corpus for the package to know what book to download. The function `gutenberg_download()` will take the `glID` as an argument and return the full text of the book as a tibble with two columns, like this one:

```{r}
gutenberg_download(1342) # 1342 is the ID for "Pride and Prejudice"
```

This tibble needs to be transformed into a single text string that fits in a column of our corpus tibble. We can use the `pull()` function to extract the text column and then use `paste(collapse = " ")` to concatenate all the lines into a single string. 

```{r}
# Download all books at once
corpus_texts <- corpus %>%
   mutate(text = map(glID, ~gutenberg_download(.x) %>%
   pull(text) %>%
   paste(collapse = " ")))

corpus_texts
```

::: {.callout-tip collapse="true" title="Understanding the `map()` function"}
The `map()` function from the `purrr` package (part of tidyverse) applies a function to each element of a vector or list. In our case:

- `map(glID, ~...)` takes each ID from the glID column
- The `~` creates an anonymous function (similar to a lambda function in Python)
- `.x` represents each individual ID as `map()` processes them one by one
- So `~gutenberg_download(.x)` means "for each ID, download that book"

This is equivalent to writing a for loop, but more concise and functional programming style. The result is that each book gets downloaded and processed, creating a new `text` column in our tibble.

If you're new to `map()`, think of it as "do this same operation to every item in this list."
:::

In the end, we will have a `corpus_texts` tibble that contains the full text of each book in our corpus as a character vector stored in the `text` column.